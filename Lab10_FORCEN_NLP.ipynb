{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importation data"
      ],
      "metadata": {
        "id": "FrWnDe2Iih1l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gledB7bpgDgk",
        "outputId": "2f1496a0-0b7f-4056-defe-c0acf1572655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     v1                                                 v2 Unnamed: 2  \\\n",
            "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
            "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
            "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
            "\n",
            "  Unnamed: 3 Unnamed: 4  \n",
            "0        NaN        NaN  \n",
            "1        NaN        NaN  \n",
            "2        NaN        NaN  \n",
            "3        NaN        NaN  \n",
            "4        NaN        NaN  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"/content/spam.csv\",encoding=\"ISO-8859-1\")\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extension de l'affichage de la colonne texte sms\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "#en utilisant uniquement les colonnes v1 et v2\n",
        "data= data [['v1','v2']]\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "ZdPCxT2UhxrU",
        "outputId": "0e177587-9508-4db9-dddf-4b324f619b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-d1256ac462e1>:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  pd.set_option('display.max_colwidth', -1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     v1  \\\n",
              "0  ham    \n",
              "1  ham    \n",
              "2  spam   \n",
              "3  ham    \n",
              "4  ham    \n",
              "\n",
              "                                                                                                                                                            v2  \n",
              "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                              \n",
              "1  Ok lar... Joking wif u oni...                                                                                                                                \n",
              "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  \n",
              "3  U dun say so early hor... U c already then say...                                                                                                            \n",
              "4  Nah I don't think he goes to usf, he lives around here though                                                                                                "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-71eca090-c7f5-4d0d-8ed6-63459fc79a36\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71eca090-c7f5-4d0d-8ed6-63459fc79a36')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-71eca090-c7f5-4d0d-8ed6-63459fc79a36 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-71eca090-c7f5-4d0d-8ed6-63459fc79a36');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-af7072a0-5436-4532-bb8b-d0fe1529c2bc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-af7072a0-5436-4532-bb8b-d0fe1529c2bc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-af7072a0-5436-4532-bb8b-d0fe1529c2bc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Gcsd3BOiU13",
        "outputId": "563c911c-cdc5-45d9-b0d8-3b542ff72011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5572, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les données comportent 5 572 lignes et 2 colonnes. Vous pouvez vérifier la forme des données à l'aide de la fonction data.shape. Vérifions la répartition des variables dépendantes entre le spam et le jambon.\n"
      ],
      "metadata": {
        "id": "AF4R7ISbiKki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#vérifier le nombre de variables dépendantes\n",
        "data['v1'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nX0VFZUiHut",
        "outputId": "68554569-b169-4a9a-ba54-2e1c2904b415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     4825\n",
              "spam    747 \n",
              "Name: v1, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Étapes pour nettoyer les données"
      ],
      "metadata": {
        "id": "czYyk1L_idHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppression de la ponctuation\n",
        "Dans cette étape, toutes les ponctuations du texte sont supprimées. La bibliothèque de chaînes de Python contient une liste prédéfinie de ponctuations telles que  '!\"#$%&'()*+,-./:;?@[\\]^_`{|}~'"
      ],
      "metadata": {
        "id": "EwhluGP0isXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#library that contains punctuation = bibliothèque contenant la chaîne\n",
        "import string\n",
        "string.punctuation\n",
        "#defining the function to remove punctuation = définition de la fonction pour supprimer la ponctuation\n",
        "def remove_punctuation(text):\n",
        "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
        "    return punctuationfree\n",
        "#storing the puntuation free text = stockage des données de texte libre de ponctuation\n",
        "data['clean_msg']= data['v2'].apply(lambda x:remove_punctuation(x))\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "zQINa0Wbi4KZ",
        "outputId": "9ad306c6-5cca-4f8b-870d-76b4098d80fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     v1  \\\n",
              "0  ham    \n",
              "1  ham    \n",
              "2  spam   \n",
              "3  ham    \n",
              "4  ham    \n",
              "\n",
              "                                                                                                                                                            v2  \\\n",
              "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                               \n",
              "1  Ok lar... Joking wif u oni...                                                                                                                                 \n",
              "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
              "3  U dun say so early hor... U c already then say...                                                                                                             \n",
              "4  Nah I don't think he goes to usf, he lives around here though                                                                                                 \n",
              "\n",
              "                                                                                                                                               clean_msg  \\\n",
              "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                                                  \n",
              "1  Ok lar Joking wif u oni                                                                                                                                 \n",
              "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s   \n",
              "3  U dun say so early hor U c already then say                                                                                                             \n",
              "4  Nah I dont think he goes to usf he lives around here though                                                                                             \n",
              "\n",
              "                                                                                                                                               msg_lower  \\\n",
              "0  go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat                                                  \n",
              "1  ok lar joking wif u oni                                                                                                                                 \n",
              "2  free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s   \n",
              "3  u dun say so early hor u c already then say                                                                                                             \n",
              "4  nah i dont think he goes to usf he lives around here though                                                                                             \n",
              "\n",
              "                                                                                                                                              msg_tokenied  \n",
              "0  [go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat]                                                 \n",
              "1  [ok lar joking wif u oni]                                                                                                                                \n",
              "2  [free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s]  \n",
              "3  [u dun say so early hor u c already then say]                                                                                                            \n",
              "4  [nah i dont think he goes to usf he lives around here though]                                                                                            "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-791783f9-13f6-4c4f-9ff2-c0343f6bd0f3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "      <th>clean_msg</th>\n",
              "      <th>msg_lower</th>\n",
              "      <th>msg_tokenied</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
              "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat</td>\n",
              "      <td>go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat</td>\n",
              "      <td>[go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "      <td>ok lar joking wif u oni</td>\n",
              "      <td>[ok lar joking wif u oni]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s</td>\n",
              "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
              "      <td>[free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "      <td>u dun say so early hor u c already then say</td>\n",
              "      <td>[u dun say so early hor u c already then say]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
              "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
              "      <td>[nah i dont think he goes to usf he lives around here though]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-791783f9-13f6-4c4f-9ff2-c0343f6bd0f3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-791783f9-13f6-4c4f-9ff2-c0343f6bd0f3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-791783f9-13f6-4c4f-9ff2-c0343f6bd0f3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-509ab5f8-0623-42ee-9a1c-faf8b6a49818\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-509ab5f8-0623-42ee-9a1c-faf8b6a49818')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-509ab5f8-0623-42ee-9a1c-faf8b6a49818 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous pouvons voir dans le résultat ci-dessus que toutes les ponctuations sont supprimées de la v2 et stockées dans la colonne clean_msg."
      ],
      "metadata": {
        "id": "NFPMWTUujdwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Abaisser le texte\n",
        "\n",
        "Il s'agit de l'une des étapes Python de prétraitement de texte les plus courantes, où le texte est converti dans la même casse, de préférence en minuscules. Mais il n'est pas nécessaire de réaliser cette étape à chaque fois que vous travaillez sur un problème NLP, car pour certains problèmes, les minuscules peuvent entraîner une perte d'informations.\n",
        "\n",
        "Par exemple, si dans un projet nous traitons des émotions d'une personne, les mots écrits en majuscules peuvent être un signe de frustration ou d'excitation."
      ],
      "metadata": {
        "id": "wBe_3lCQjtud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['msg_lower']= data['clean_msg'].apply(lambda x : x.lower())"
      ],
      "metadata": {
        "id": "Bzjq-fbIj1me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "GpB2yogrzEqB",
        "outputId": "192df376-634f-414c-bbdb-aafd09bf0ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     v1  \\\n",
              "0  ham    \n",
              "1  ham    \n",
              "2  spam   \n",
              "3  ham    \n",
              "4  ham    \n",
              "\n",
              "                                                                                                                                                            v2  \\\n",
              "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                               \n",
              "1  Ok lar... Joking wif u oni...                                                                                                                                 \n",
              "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
              "3  U dun say so early hor... U c already then say...                                                                                                             \n",
              "4  Nah I don't think he goes to usf, he lives around here though                                                                                                 \n",
              "\n",
              "                                                                                                                                               clean_msg  \\\n",
              "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                                                  \n",
              "1  Ok lar Joking wif u oni                                                                                                                                 \n",
              "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s   \n",
              "3  U dun say so early hor U c already then say                                                                                                             \n",
              "4  Nah I dont think he goes to usf he lives around here though                                                                                             \n",
              "\n",
              "                                                                                                                                               msg_lower  \n",
              "0  go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat                                                 \n",
              "1  ok lar joking wif u oni                                                                                                                                \n",
              "2  free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s  \n",
              "3  u dun say so early hor u c already then say                                                                                                            \n",
              "4  nah i dont think he goes to usf he lives around here though                                                                                            "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-17beec4e-9fb7-4957-8f61-18160f0ee947\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "      <th>clean_msg</th>\n",
              "      <th>msg_lower</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
              "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat</td>\n",
              "      <td>go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "      <td>ok lar joking wif u oni</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s</td>\n",
              "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "      <td>u dun say so early hor u c already then say</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
              "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17beec4e-9fb7-4957-8f61-18160f0ee947')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-17beec4e-9fb7-4957-8f61-18160f0ee947 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-17beec4e-9fb7-4957-8f61-18160f0ee947');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4498da9d-e36a-4264-9e8b-e04aa56689f5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4498da9d-e36a-4264-9e8b-e04aa56689f5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4498da9d-e36a-4264-9e8b-e04aa56689f5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenisation\n",
        "\n",
        "Dans cette étape, le texte est divisé en unités plus petites. Nous pouvons utiliser la tokenisation de phrases ou la tokenisation de mots en fonction de notre énoncé du problème.\n"
      ],
      "metadata": {
        "id": "K76fswBnkBUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining function for tokenization ==fonction de définition pour la tokenisation\n",
        "import re\n",
        "def tokenization(text):\n",
        "    tokens = re.split('W+',text)\n",
        "    return tokens\n",
        "#applying function to the column == application de la fonction à la colonne\n",
        "data['msg_tokenied']= data['msg_lower'].apply(lambda x: tokenization(x))"
      ],
      "metadata": {
        "id": "T_nAq62zkTvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "7b2wmHrmkfuI",
        "outputId": "2e052cfa-ab01-4fa7-f291-dcc0d9c5ad6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     v1  \\\n",
              "0  ham    \n",
              "1  ham    \n",
              "2  spam   \n",
              "3  ham    \n",
              "4  ham    \n",
              "\n",
              "                                                                                                                                                            v2  \\\n",
              "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                               \n",
              "1  Ok lar... Joking wif u oni...                                                                                                                                 \n",
              "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
              "3  U dun say so early hor... U c already then say...                                                                                                             \n",
              "4  Nah I don't think he goes to usf, he lives around here though                                                                                                 \n",
              "\n",
              "                                                                                                                                               clean_msg  \\\n",
              "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                                                  \n",
              "1  Ok lar Joking wif u oni                                                                                                                                 \n",
              "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s   \n",
              "3  U dun say so early hor U c already then say                                                                                                             \n",
              "4  Nah I dont think he goes to usf he lives around here though                                                                                             \n",
              "\n",
              "                                                                                                                                               msg_lower  \\\n",
              "0  go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat                                                  \n",
              "1  ok lar joking wif u oni                                                                                                                                 \n",
              "2  free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s   \n",
              "3  u dun say so early hor u c already then say                                                                                                             \n",
              "4  nah i dont think he goes to usf he lives around here though                                                                                             \n",
              "\n",
              "                                                                                                                                              msg_tokenied  \n",
              "0  [go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat]                                                 \n",
              "1  [ok lar joking wif u oni]                                                                                                                                \n",
              "2  [free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s]  \n",
              "3  [u dun say so early hor u c already then say]                                                                                                            \n",
              "4  [nah i dont think he goes to usf he lives around here though]                                                                                            "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7dd520c1-0862-4e5c-8bac-90ddce3d7895\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "      <th>clean_msg</th>\n",
              "      <th>msg_lower</th>\n",
              "      <th>msg_tokenied</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
              "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat</td>\n",
              "      <td>go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat</td>\n",
              "      <td>[go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "      <td>ok lar joking wif u oni</td>\n",
              "      <td>[ok lar joking wif u oni]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s</td>\n",
              "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
              "      <td>[free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "      <td>u dun say so early hor u c already then say</td>\n",
              "      <td>[u dun say so early hor u c already then say]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
              "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
              "      <td>[nah i dont think he goes to usf he lives around here though]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7dd520c1-0862-4e5c-8bac-90ddce3d7895')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7dd520c1-0862-4e5c-8bac-90ddce3d7895 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7dd520c1-0862-4e5c-8bac-90ddce3d7895');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-197e5631-ccee-47bb-8aa1-8238002f8002\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-197e5631-ccee-47bb-8aa1-8238002f8002')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-197e5631-ccee-47bb-8aa1-8238002f8002 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Résultat : les phrases sont transformées en mots."
      ],
      "metadata": {
        "id": "VmMQAv5UlQin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Suppression du mot d'arrêt\n",
        "\n",
        "Les mots vides sont les mots couramment utilisés et sont supprimés du texte car ils n’ajoutent aucune valeur à l’analyse. Ces mots ont peu ou pas de sens.\n",
        "\n",
        "La bibliothèque NLTK consiste en une liste de mots considérés comme des mots vides pour la langue anglaise. Certains d'entre eux sont : [ je, moi, mon, moi-même, nous, notre, le nôtre, nous-mêmes, vous, vous êtes, vous avez, vous allez, vous feriez, votre, le vôtre, vous-même, vous-mêmes, lui,  la plupart, d'autres, certains, tels, non, ni, pas, seulement, possèdent, pareil, donc, aussi, très, s, t, peuvent, volonté, juste, ne, ne pas, devraient, auraient dû, maintenant, d, ll, m, o, re, ve, y, ain, ne sont pas, pouvaient, ne pouvaient pas, ne l'ont pas fait, ne l'ont pas]\n",
        "\n",
        "Mais il n’est pas nécessaire d’utiliser la liste fournie comme mots vides car ils doivent être choisis judicieusement en fonction du projet. Par exemple , « Comment » peut être un mot vide pour un modèle mais peut être important pour un autre problème dans lequel nous travaillons sur les requêtes des clients.  Nous pouvons créer une liste personnalisée de mots vides pour différents problèmes."
      ],
      "metadata": {
        "id": "RcZtf0Q_lRY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSg7-a5YmHES",
        "outputId": "7139720d-8e8d-4944-8f56-1ec289b6c8e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDketWGqmiMr",
        "outputId": "cb9c8bd5-8292-4fb8-fab8-76bf68243624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "HHy6lMOqCgjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing nlp library\n",
        "import nltk\n",
        "#Stop words present in the library == Mots vides présents dans la bibliothèque\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stopwords[0:10]\n",
        "['i', \"he\", \"to\", \"here\", 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
        "#defining the function to remove stopwords from tokenized text == définir la fonction pour supprimer les mots vides du texte tokenisé\n",
        "def remove_stopwords(text):\n",
        "    output= [i for i in text if i not in stopwords]\n",
        "    return output\n",
        "#applying the function ==application de la fonction\n",
        "data['no_stopwords']= data['msg_tokenied'].apply(lambda x:remove_stopwords(x))"
      ],
      "metadata": {
        "id": "6vm21oOnltuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "wxxEv6LtoRgn",
        "outputId": "cc0893a4-1454-4b36-e042-2feab520919b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     v1  \\\n",
              "0  ham    \n",
              "1  ham    \n",
              "2  spam   \n",
              "3  ham    \n",
              "4  ham    \n",
              "\n",
              "                                                                                                                                                            v2  \\\n",
              "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                               \n",
              "1  Ok lar... Joking wif u oni...                                                                                                                                 \n",
              "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
              "3  U dun say so early hor... U c already then say...                                                                                                             \n",
              "4  Nah I don't think he goes to usf, he lives around here though                                                                                                 \n",
              "\n",
              "                                                                                                                                               clean_msg  \\\n",
              "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                                                  \n",
              "1  Ok lar Joking wif u oni                                                                                                                                 \n",
              "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s   \n",
              "3  U dun say so early hor U c already then say                                                                                                             \n",
              "4  Nah I dont think he goes to usf he lives around here though                                                                                             \n",
              "\n",
              "                                                                                                                                               msg_lower  \\\n",
              "0  go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat                                                  \n",
              "1  ok lar joking wif u oni                                                                                                                                 \n",
              "2  free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s   \n",
              "3  u dun say so early hor u c already then say                                                                                                             \n",
              "4  nah i dont think he goes to usf he lives around here though                                                                                             \n",
              "\n",
              "                                                                                                                                              msg_tokenied  \\\n",
              "0  [go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat]                                                  \n",
              "1  [ok lar joking wif u oni]                                                                                                                                 \n",
              "2  [free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s]   \n",
              "3  [u dun say so early hor u c already then say]                                                                                                             \n",
              "4  [nah i dont think he goes to usf he lives around here though]                                                                                             \n",
              "\n",
              "                                                                                                                                              no_stopwords  \n",
              "0  [go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat]                                                 \n",
              "1  [ok lar joking wif u oni]                                                                                                                                \n",
              "2  [free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s]  \n",
              "3  [u dun say so early hor u c already then say]                                                                                                            \n",
              "4  [nah i dont think he goes to usf he lives around here though]                                                                                            "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4d86b156-c61b-4146-b6f3-97841f2ce957\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "      <th>clean_msg</th>\n",
              "      <th>msg_lower</th>\n",
              "      <th>msg_tokenied</th>\n",
              "      <th>no_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
              "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat</td>\n",
              "      <td>go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat</td>\n",
              "      <td>[go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat]</td>\n",
              "      <td>[go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "      <td>ok lar joking wif u oni</td>\n",
              "      <td>[ok lar joking wif u oni]</td>\n",
              "      <td>[ok lar joking wif u oni]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s</td>\n",
              "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
              "      <td>[free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s]</td>\n",
              "      <td>[free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "      <td>u dun say so early hor u c already then say</td>\n",
              "      <td>[u dun say so early hor u c already then say]</td>\n",
              "      <td>[u dun say so early hor u c already then say]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
              "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
              "      <td>[nah i dont think he goes to usf he lives around here though]</td>\n",
              "      <td>[nah i dont think he goes to usf he lives around here though]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d86b156-c61b-4146-b6f3-97841f2ce957')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4d86b156-c61b-4146-b6f3-97841f2ce957 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4d86b156-c61b-4146-b6f3-97841f2ce957');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e1d79461-9664-4cb5-a284-144145cbc0c0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e1d79461-9664-4cb5-a284-144145cbc0c0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e1d79461-9664-4cb5-a284-144145cbc0c0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sortie : Les mots vides présents dans la bibliothèque nltk, tels que in, jusqu'à, to, I, here, sont supprimés du texte tokenisé et le reste est stocké dans la colonne no_stopwords."
      ],
      "metadata": {
        "id": "x8Ug0ytnluGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dérivé/ Stemming ou Radicalisation\n",
        "\n",
        "Elle est également connue sous le nom d'étape de normalisation du texte, où les mots sont réduits ou réduits jusqu'à leur forme racine/base.   Par exemple , des mots comme « programmeur », « programmation », « programme » seront issus de « programme ».\n",
        "\n",
        "Mais l’ inconvénient de la radicalisation est qu’elle dérive les mots de telle sorte que sa forme racine perd son sens ou qu’elle ne soit pas réduite à un mot anglais approprié. Nous verrons cela dans les étapes effectuées ci-dessous."
      ],
      "metadata": {
        "id": "bumHFzL3puUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the Stemming function from nltk library ==  #importation de la fonction Stemming depuis la bibliothèque nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "#defining the object for stemming  == définition de l'objet pour la racine\n",
        "porter_stemmer = PorterStemmer()\n",
        "#defining a function for stemming == définir une fonction pour radicaliser\n",
        "def stemming(text):\n",
        "  stem_text = [porter_stemmer.stem(word) for word in text]\n",
        "  return stem_text\n",
        "  data['msg_stemmed']=data['no_sw_msg'].apply(lambda x: stemming(x))"
      ],
      "metadata": {
        "id": "7xCekysOp8YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sortie : Dans l'image ci-dessous, nous pouvons voir comment certains mots dérivent de leur base.\n",
        "\n",
        "fou -> fou\n",
        "\n",
        "disponible -> disponible\n",
        "\n",
        "entrée-> entrée\n",
        "\n",
        "début -> début"
      ],
      "metadata": {
        "id": "wMdFtkI0q0iG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatisation\n",
        "\n",
        "Il racine le mot mais veille à ce qu'il ne perde pas son sens. La lemmatisation dispose d'un dictionnaire prédéfini qui stocke le contexte des mots et vérifie le mot dans le dictionnaire tout en le diminuant.\n",
        "\n",
        "La différence entre le stemming et la lemmatisation peut être comprise avec l'exemple fourni ci-dessous.\n",
        "\n",
        "Mot original\tAprès la tige\tAprès lemmatisation\n",
        "oie\tbon\toie\n",
        "oies\tbon sang\toie"
      ],
      "metadata": {
        "id": "sWucJoaexTmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "#defining the object for Lemmatization\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "#defining the function for lemmatization\n",
        "def lemmatizer(text):\n",
        "  lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
        "  return lemm_text\n",
        "  data['msg_lemmatized']=data['no_stopwords'].apply(lambda x:lemmatizer(x))"
      ],
      "metadata": {
        "id": "tKwbsNhdxvLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Résultat : La différence entre la recherche de racines et la lemmatisation peut être vue dans le résultat ci-dessous.\n",
        "\n",
        "Dans la première rangée, crazy a été remplacé par fou , ce qui n'a aucun sens mais pour la lemmatisation, c'est resté le même, c'est-à-dire fou\n",
        "\n",
        "Dans la dernière rangée, go a changé pour go en radical mais pour la lemmatisation, il s'est converti en go , ce qui est significatif."
      ],
      "metadata": {
        "id": "LyAt-Oo-9_hV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une fois toutes les étapes de traitement de texte effectuées, les données finales acquises sont converties sous forme numérique à l'aide de Bag of Words ou TF-IDF."
      ],
      "metadata": {
        "id": "RfNuVFsH-HgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Méthodes pour effectuer la tokenisation en Python"
      ],
      "metadata": {
        "id": "CnkGfiIdQT2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenisation à l'aide de la fonction split() de Python"
      ],
      "metadata": {
        "id": "U7X3MAveQ0-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Commençons par la méthode split() car c'est la plus basique. Il renvoie une liste de chaînes après avoir divisé la chaîne donnée par le séparateur spécifié. Par défaut, split() casse une chaîne à chaque espace. Nous pouvons changer le séparateur en n'importe quoi. Regardons ça.\n",
        "\n",
        "##Tokenisation de mots"
      ],
      "metadata": {
        "id": "uZ_GiwGvQ6FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed\n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "# Splits at space\n",
        "text.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqhQASr7Q2K2",
        "outputId": "a783da56-43ca-47ac-e5f5-3d3370aaf83e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Founded',\n",
              " 'in',\n",
              " '2002,',\n",
              " 'SpaceX’s',\n",
              " 'mission',\n",
              " 'is',\n",
              " 'to',\n",
              " 'enable',\n",
              " 'humans',\n",
              " 'to',\n",
              " 'become',\n",
              " 'a',\n",
              " 'spacefaring',\n",
              " 'civilization',\n",
              " 'and',\n",
              " 'a',\n",
              " 'multi-planet',\n",
              " 'species',\n",
              " 'by',\n",
              " 'building',\n",
              " 'a',\n",
              " 'self-sustaining',\n",
              " 'city',\n",
              " 'on',\n",
              " 'Mars.',\n",
              " 'In',\n",
              " '2008,',\n",
              " 'SpaceX’s',\n",
              " 'Falcon',\n",
              " '1',\n",
              " 'became',\n",
              " 'the',\n",
              " 'first',\n",
              " 'privately',\n",
              " 'developed',\n",
              " 'liquid-fuel',\n",
              " 'launch',\n",
              " 'vehicle',\n",
              " 'to',\n",
              " 'orbit',\n",
              " 'the',\n",
              " 'Earth.']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenisation des phrases\n",
        "\n",
        "Ceci est similaire à la tokenisation des mots. Ici, nous étudions la structure des phrases dans l’analyse. Une phrase se termine généralement par un point (.), on peut donc utiliser « ». comme séparateur pour casser la chaîne :"
      ],
      "metadata": {
        "id": "g0pEo6vdRQOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed\n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "# Splits at '.'\n",
        "text.split('. ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t2vY3jYRSgZ",
        "outputId": "dd9c57b4-8268-4af3-e0f1-e6e5d92f0e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\\nspecies by building a self-sustaining city on Mars',\n",
              " 'In 2008, SpaceX’s Falcon 1 became the first privately developed\\nliquid-fuel launch vehicle to orbit the Earth.']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un inconvénient majeur de l’utilisation de la méthode split() de Python est que nous ne pouvons utiliser qu’un seul séparateur à la fois. Autre chose à noter : dans la tokenisation des mots, split() ne considérait pas la ponctuation comme un jeton distinct."
      ],
      "metadata": {
        "id": "1VPu0z0MRsGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenisation à l'aide d'expressions régulières (RegEx)\n",
        "\n",
        "Tout d’abord, comprenons ce qu’est une expression régulière. Il s'agit essentiellement d'une séquence de caractères spéciaux qui vous aide à faire correspondre ou à trouver d'autres chaînes ou ensembles de chaînes en utilisant cette séquence comme modèle.\n",
        "\n",
        "Nous pouvons utiliser la bibliothèque re en Python pour travailler avec des expressions régulières. Cette bibliothèque est préinstallée avec le package d'installation Python.\n",
        "\n",
        "Maintenant, effectuons la tokenisation des mots et la tokenisation des phrases en gardant RegEx à l'esprit.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MrhWQ-8PRv6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed\n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "tokens = re.findall(\"[\\w']+\", text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbGhMRj-Rwag",
        "outputId": "d601bd94-83f8-4185-dc90-1e1df5940f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Founded', 'in', '2002', 'SpaceX', 's', 'mission', 'is', 'to', 'enable', 'humans', 'to', 'become', 'a', 'spacefaring', 'civilization', 'and', 'a', 'multi', 'planet', 'species', 'by', 'building', 'a', 'self', 'sustaining', 'city', 'on', 'Mars', 'In', '2008', 'SpaceX', 's', 'Falcon', '1', 'became', 'the', 'first', 'privately', 'developed', 'liquid', 'fuel', 'launch', 'vehicle', 'to', 'orbit', 'the', 'Earth']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La fonction re.findall() trouve tous les mots qui correspondent au modèle qui lui est transmis et les stocke dans la liste.\n",
        "\n",
        "Le «\\w » représente « n'importe quel caractère de mot », ce qui signifie généralement alphanumérique (lettres, chiffres) et trait de soulignement (_). «+» signifie n'importe quel nombre de fois. Ainsi, [\\w']+ signale que le code doit trouver tous les caractères alphanumériques jusqu'à ce qu'un autre caractère soit rencontré."
      ],
      "metadata": {
        "id": "6zH5ceEPSZs8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenisation des phrases\n",
        "\n",
        "Pour effectuer la tokenisation des phrases, nous pouvons utiliser la fonction re.split() . Cela divisera le texte en phrases en y passant un modèle."
      ],
      "metadata": {
        "id": "J5iMRGA2SkSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\n",
        "species by building a self-sustaining city on, Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed\n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "sentences = re.compile('[.!?] ').split(text)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XC7A1JJKSph7",
        "outputId": "550b49eb-541f-40d8-d49b-0858a8357ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\\nspecies by building a self-sustaining city on, Mars',\n",
              " 'In 2008, SpaceX’s Falcon 1 became the first privately developed\\nliquid-fuel launch vehicle to orbit the Earth.']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici, nous avons un avantage sur la méthode split() car nous pouvons passer plusieurs séparateurs en même temps. Dans le code ci-dessus, nous avons utilisé la fonction re.compile() dans laquelle nous avons passé [.?!]. Cela signifie que les phrases seront divisées dès que l'un de ces caractères sera rencontré."
      ],
      "metadata": {
        "id": "Vm-mToNoS7ME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenisation à l'aide de NLTK\n",
        "\n",
        "C’est désormais une bibliothèque que vous apprécierez au fur et à mesure que vous travaillerez avec des données textuelles. NLTK, abréviation de Natural Language ToolKit, est une bibliothèque écrite en Python pour le traitement symbolique et statistique du langage naturel."
      ],
      "metadata": {
        "id": "QXoQG666TCm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installons NLTK en utilisant le code ci-dessous"
      ],
      "metadata": {
        "id": "dErNHLLsTIW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --user -U nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPcDcGHbTo7T",
        "outputId": "c04190e5-ee0a-4d9e-a97c-09c4c5144254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK** contient un module appelé tokenize() qui se classe en deux sous-catégories:\n",
        "\n",
        "**Word tokenize** : nous utilisons la méthode word_tokenize() pour diviser une phrase en jetons ou en mots\n",
        "\n",
        "**Sentence tokenize** : nous utilisons la méthode sent_tokenize() pour diviser un document ou un paragraphe en phrases"
      ],
      "metadata": {
        "id": "JRJRCRe6T079"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voyons ces deux éléments un par un.\n",
        "\n",
        "###Tokenisation de mots"
      ],
      "metadata": {
        "id": "SMHdxmRpUOVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "eFcJc1X7UmiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPvoJGN_UqoA",
        "outputId": "64fab06b-45c9-430a-9342-80ca311c2a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed\n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxFYKURsUWBa",
        "outputId": "2603a627-8779-4aa5-b8d6-73a2c662e3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Founded',\n",
              " 'in',\n",
              " '2002',\n",
              " ',',\n",
              " 'SpaceX',\n",
              " '’',\n",
              " 's',\n",
              " 'mission',\n",
              " 'is',\n",
              " 'to',\n",
              " 'enable',\n",
              " 'humans',\n",
              " 'to',\n",
              " 'become',\n",
              " 'a',\n",
              " 'spacefaring',\n",
              " 'civilization',\n",
              " 'and',\n",
              " 'a',\n",
              " 'multi-planet',\n",
              " 'species',\n",
              " 'by',\n",
              " 'building',\n",
              " 'a',\n",
              " 'self-sustaining',\n",
              " 'city',\n",
              " 'on',\n",
              " 'Mars',\n",
              " '.',\n",
              " 'In',\n",
              " '2008',\n",
              " ',',\n",
              " 'SpaceX',\n",
              " '’',\n",
              " 's',\n",
              " 'Falcon',\n",
              " '1',\n",
              " 'became',\n",
              " 'the',\n",
              " 'first',\n",
              " 'privately',\n",
              " 'developed',\n",
              " 'liquid-fuel',\n",
              " 'launch',\n",
              " 'vehicle',\n",
              " 'to',\n",
              " 'orbit',\n",
              " 'the',\n",
              " 'Earth',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remarquez comment NLTK considère la ponctuation comme un jeton ? Par conséquent, pour les tâches futures, nous devons supprimer les ponctuations de la liste initiale.\n"
      ],
      "metadata": {
        "id": "SH0KXAlNU14m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenisation des phrases"
      ],
      "metadata": {
        "id": "L8T3zFyNU9DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed\n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD6g_lavUfRv",
        "outputId": "50816808-2a6e-46ef-cfb0-b047956d84a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\\nspecies by building a self-sustaining city on Mars.',\n",
              " 'In 2008, SpaceX’s Falcon 1 became the first privately developed\\nliquid-fuel launch vehicle to orbit the Earth.']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenisation à l'aide de la bibliothèque spaCy\n",
        "\n",
        "J'adore la bibliothèque spaCy. Je ne me souviens pas de la dernière fois où je ne l'ai pas utilisé alors que je travaillais sur un projet PNL. C'est tellement utile.\n",
        "\n",
        "spaCy est une bibliothèque open source pour le traitement avancé du langage naturel (NLP) . Il prend en charge plus de 49 langues et offre une vitesse de calcul de pointe."
      ],
      "metadata": {
        "id": "jOB3k5sNVe0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intsallation  spaCy"
      ],
      "metadata": {
        "id": "tIB22y1JWB8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avec CPU"
      ],
      "metadata": {
        "id": "S6S-I32iWL5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install -c conda-forge spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "c1oXV-rlWtI8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a110c43-3465-4b2a-cef6-680ca47728f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: conda: command not found\n",
            "2024-02-02 01:38:41.647640: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-02 01:38:41.647702: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-02 01:38:41.649183: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-02 01:38:41.657912: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-02-02 01:38:42.879392: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.14)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.4)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avec GPU"
      ],
      "metadata": {
        "id": "rH3_apXjWPBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#conda install -c conda-forge spacy\n",
        "!conda install -c conda-forge spacy\n",
        "!conda install -c conda-forge cupy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "Rg_wASYAWSci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Avec Pip install"
      ],
      "metadata": {
        "id": "wx7MB5zGWvJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy"
      ],
      "metadata": {
        "id": "skKeAyaXXo67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "26b1a7e0-5a16-435b-fd17-8b1eabbd8194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-69.0.3-py3-none-any.whl (819 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.5/819.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-23.3.2 setuptools-69.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.14)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (69.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.4)\n",
            "Downloading spacy-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cloudpathlib, weasel, spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.6.1\n",
            "    Uninstalling spacy-3.6.1:\n",
            "      Successfully uninstalled spacy-3.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.7.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloudpathlib-0.16.0 spacy-3.7.2 weasel-0.3.4\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m venv .env\n",
        "!source .env/bin/activate\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy"
      ],
      "metadata": {
        "id": "rgp9xrOdXENp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ec7370-55c9-4066-9f9d-095945e482cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The virtual environment was not created successfully because ensurepip is not\n",
            "available.  On Debian/Ubuntu systems, you need to install the python3-venv\n",
            "package using the following command.\n",
            "\n",
            "    apt install python3.10-venv\n",
            "\n",
            "You may need to use sudo with that command.  After installing the python3-venv\n",
            "package, recreate your virtual environment.\n",
            "\n",
            "Failing command: /content/.env/bin/python3\n",
            "\n",
            "/bin/bash: line 1: .env/bin/activate: No such file or directory\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (69.0.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.14)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (69.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMZECk2UXEn1",
        "outputId": "e806f8c1-cca6-4230-98f4-c1d208df2291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (69.0.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.14)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (69.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m2024-02-02 01:40:32.231115: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-02 01:40:32.231236: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-02 01:40:32.232990: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-02 01:40:32.241523: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-02-02 01:40:33.473686: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.14)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.4)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 3.6.0\n",
            "    Uninstalling en-core-web-sm-3.6.0:\n",
            "      Successfully uninstalled en-core-web-sm-3.6.0\n",
            "Successfully installed en-core-web-sm-3.7.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenisation de mots"
      ],
      "metadata": {
        "id": "yLM6FPf7X8vR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed\n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "\n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "my_doc = nlp(text)\n",
        "\n",
        "# Create list of word tokens\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "    token_list.append(token.text)\n",
        "token_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bJhx3OrX_vY",
        "outputId": "51467a53-4670-443c-efb8-bec40b563d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Founded',\n",
              " 'in',\n",
              " '2002',\n",
              " ',',\n",
              " 'SpaceX',\n",
              " '’s',\n",
              " 'mission',\n",
              " 'is',\n",
              " 'to',\n",
              " 'enable',\n",
              " 'humans',\n",
              " 'to',\n",
              " 'become',\n",
              " 'a',\n",
              " 'spacefaring',\n",
              " 'civilization',\n",
              " 'and',\n",
              " 'a',\n",
              " 'multi',\n",
              " '-',\n",
              " 'planet',\n",
              " '\\n',\n",
              " 'species',\n",
              " 'by',\n",
              " 'building',\n",
              " 'a',\n",
              " 'self',\n",
              " '-',\n",
              " 'sustaining',\n",
              " 'city',\n",
              " 'on',\n",
              " 'Mars',\n",
              " '.',\n",
              " 'In',\n",
              " '2008',\n",
              " ',',\n",
              " 'SpaceX',\n",
              " '’s',\n",
              " 'Falcon',\n",
              " '1',\n",
              " 'became',\n",
              " 'the',\n",
              " 'first',\n",
              " 'privately',\n",
              " 'developed',\n",
              " '\\n',\n",
              " 'liquid',\n",
              " '-',\n",
              " 'fuel',\n",
              " 'launch',\n",
              " 'vehicle',\n",
              " 'to',\n",
              " 'orbit',\n",
              " 'the',\n",
              " 'Earth',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenisation des phrases"
      ],
      "metadata": {
        "id": "bUGSwHjoYMZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "# Create the pipeline 'sentencizer' component\n",
        "sbd = nlp.create_pipe('sentencizer')\n",
        "\n",
        "# Add the component to the pipeline\n",
        "nlp.add_pipe(sbd)\n",
        "\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed\n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "\n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "doc = nlp(text)\n",
        "\n",
        "# create list of sentence tokens\n",
        "sents_list = []\n",
        "for sent in doc.sents:\n",
        "  sents_list.append(sent.text)\n",
        "  sents_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "hYrGsVKlYLoi",
        "outputId": "3741ab5b-d1e6-4da3-f8db-f5e0ce8947d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "[E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy.pipeline.sentencizer.Sentencizer object at 0x7e02f06b0940> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-cccf2581a4bd>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Add the component to the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msbd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0mbad_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE966\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbad_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfactory_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy.pipeline.sentencizer.Sentencizer object at 0x7e02f06b0940> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras Tokenisation\n",
        "Allons craquer. Pour effectuer la tokenisation de mots à l'aide de Keras, nous utilisons la méthode text_to_word_sequence de la classe keras.preprocessing.text ."
      ],
      "metadata": {
        "id": "-Ya2sw5Pj_XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-QNZ_8NkJpz",
        "outputId": "606f3a02-7331-42cb-e809-2989b8eeb5ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenisation de mots avec keras"
      ],
      "metadata": {
        "id": "I_FHcnZSkWtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# define\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed\n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "# tokenize\n",
        "result = text_to_word_sequence(text)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeixAh33kRea",
        "outputId": "4f61606a-f24c-479b-b6ca-c2338ab3226c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['founded',\n",
              " 'in',\n",
              " '2002',\n",
              " 'spacex’s',\n",
              " 'mission',\n",
              " 'is',\n",
              " 'to',\n",
              " 'enable',\n",
              " 'humans',\n",
              " 'to',\n",
              " 'become',\n",
              " 'a',\n",
              " 'spacefaring',\n",
              " 'civilization',\n",
              " 'and',\n",
              " 'a',\n",
              " 'multi',\n",
              " 'planet',\n",
              " 'species',\n",
              " 'by',\n",
              " 'building',\n",
              " 'a',\n",
              " 'self',\n",
              " 'sustaining',\n",
              " 'city',\n",
              " 'on',\n",
              " 'mars',\n",
              " 'in',\n",
              " '2008',\n",
              " 'spacex’s',\n",
              " 'falcon',\n",
              " '1',\n",
              " 'became',\n",
              " 'the',\n",
              " 'first',\n",
              " 'privately',\n",
              " 'developed',\n",
              " 'liquid',\n",
              " 'fuel',\n",
              " 'launch',\n",
              " 'vehicle',\n",
              " 'to',\n",
              " 'orbit',\n",
              " 'the',\n",
              " 'earth']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras réduit la casse de tous les alphabets avant de les symboliser. Cela nous fait gagner pas mal de temps comme vous pouvez l’imaginer !"
      ],
      "metadata": {
        "id": "uraYPZjZkjzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenisation à l'aide de Gensim\n",
        "\n",
        "La dernière méthode de tokenisation que nous aborderons ici consiste à utiliser la bibliothèque Gensim. Il s'agit d'une bibliothèque open source pour la modélisation de sujets non supervisée et le traitement du langage naturel et est conçue pour extraire automatiquement des sujets sémantiques d'un document donné.\n",
        "\n",
        "Voici comment installer Gensim :"
      ],
      "metadata": {
        "id": "cHdvFCYZkuUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7WqeqxjklvS",
        "outputId": "4faf87c6-e1ea-4b5a-c1a4-5527b8365610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous pouvons utiliser la classe gensim.utils pour importer la méthode tokenize afin d'effectuer la tokenisation de mots.\n",
        "\n",
        "## Tokenisation de mots"
      ],
      "metadata": {
        "id": "7kbeQgpFlCF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import tokenize\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed\n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "list(tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7KiUy5klH3h",
        "outputId": "b7e03c4e-3dc8-41a9-d658-df774fb81990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Founded',\n",
              " 'in',\n",
              " 'SpaceX',\n",
              " 's',\n",
              " 'mission',\n",
              " 'is',\n",
              " 'to',\n",
              " 'enable',\n",
              " 'humans',\n",
              " 'to',\n",
              " 'become',\n",
              " 'a',\n",
              " 'spacefaring',\n",
              " 'civilization',\n",
              " 'and',\n",
              " 'a',\n",
              " 'multi',\n",
              " 'planet',\n",
              " 'species',\n",
              " 'by',\n",
              " 'building',\n",
              " 'a',\n",
              " 'self',\n",
              " 'sustaining',\n",
              " 'city',\n",
              " 'on',\n",
              " 'Mars',\n",
              " 'In',\n",
              " 'SpaceX',\n",
              " 's',\n",
              " 'Falcon',\n",
              " 'became',\n",
              " 'the',\n",
              " 'first',\n",
              " 'privately',\n",
              " 'developed',\n",
              " 'liquid',\n",
              " 'fuel',\n",
              " 'launch',\n",
              " 'vehicle',\n",
              " 'to',\n",
              " 'orbit',\n",
              " 'the',\n",
              " 'Earth']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenisation des phrases\n",
        "\n",
        "Pour effectuer la tokenisation des phrases, nous utilisons la méthode split_sentences de la classe gensim.summerization.texttcleaner :"
      ],
      "metadata": {
        "id": "_cWIxGk3mWf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install \"gensim==4.3.2\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOD29GDLEKML",
        "outputId": "80da7a0d-e957-41f9-9afb-881f7346740e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.3.2 in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.2) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.2) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.2) (6.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.summarization.textcleaner import split_sentences\n",
        "\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed\n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "result = split_sentences(text)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "NR5PQxHdmccy",
        "outputId": "62af91d3-784a-4d7b-cd57-1fc43849f07f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim.summarization'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-4c10391f6f9b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextcleaner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msplit_sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\n\u001b[1;32m      4\u001b[0m \u001b[0mspecies\u001b[0m \u001b[0mby\u001b[0m \u001b[0mbuilding\u001b[0m \u001b[0ma\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msustaining\u001b[0m \u001b[0mcity\u001b[0m \u001b[0mon\u001b[0m \u001b[0mMars\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIn\u001b[0m \u001b[0;36m2008\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpaceX\u001b[0m\u001b[0;31m’\u001b[0m\u001b[0ms\u001b[0m \u001b[0mFalcon\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0mbecame\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfirst\u001b[0m \u001b[0mprivately\u001b[0m \u001b[0mdeveloped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#T1. Comment tokeniser en PNL en Python ?\n",
        "\n",
        "R. En Python, la tokenisation en NLP peut être réalisée à l'aide de diverses bibliothèques telles que NLTK, SpaCy ou le module de tokenisation de la bibliothèque Transformers. Ces bibliothèques offrent des fonctions permettant de diviser le texte en jetons, tels que des mots ou des sous-mots, en fonction de différentes règles et considérations spécifiques à la langue. La tokenisation joue un rôle crucial dans diverses tâches NLP, notamment le prétraitement du texte et l'extraction de fonctionnalités."
      ],
      "metadata": {
        "id": "RYeOWmqHm-CW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2. Comment créer un jeton en Python ?\n",
        "\n",
        "A. Pour créer des jetons en Python, vous pouvez utiliser la split()méthode disponible pour les chaînes, qui divise une chaîne en une liste de sous-chaînes en fonction d'un délimiteur spécifié. Par exemple, pour symboliser une phrase en mots individuels :"
      ],
      "metadata": {
        "id": "gne5pdivnFcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Hello, how are you?\"\n",
        "tokens = sentence.split()\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7Nm5vAXnMW9",
        "outputId": "93f3b7c7-48fa-4b97-b376-e5a26cf160e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', 'how', 'are', 'you?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous pouvons prétraiter davantage les jetons en supprimant la ponctuation, en les convertissant en minuscules ou en appliquant d'autres transformations selon vos besoins."
      ],
      "metadata": {
        "id": "EeSVk20Knc9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NLP Essentials : suppression des mots vides et opération de normalisation du texte à l'aide de NLTK et spaCy en Python"
      ],
      "metadata": {
        "id": "V70r9Bfe7gnp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Que sont les mots vides ?\n",
        "\n",
        "Les mots vides sont les mots les plus courants dans n’importe quelle langue naturelle. Dans le but d'analyser des données textuelles et de créer des modèles PNL, ces mots vides peuvent ne pas ajouter beaucoup de valeur à la signification du document.\n",
        "\n",
        "Généralement, les mots les plus couramment utilisés dans un texte sont « le », « est », « dans », « pour », « où », « quand », « à », « à », etc.\n",
        "\n",
        "Considérez cette chaîne de texte – « Il y a un stylo sur la table ». Désormais, les mots « est », « un », « sur » et « le » n'ajoutent aucun sens à la déclaration lors de son analyse. Alors que des mots comme « là », « livre » et « table » sont les mots-clés et nous disent de quoi parle la déclaration."
      ],
      "metadata": {
        "id": "uzmaSKwq7oSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pourquoi devons-nous supprimer les mots vides ?\n",
        "La suppression des mots vides n’est pas une règle absolue en PNL. Cela dépend de la tâche sur laquelle nous travaillons. Pour des tâches telles que la classification de texte, où le texte doit être classé en différentes catégories, les mots vides sont supprimés ou exclus du texte donné afin que davantage d'attention puisse être accordée aux mots qui définissent le sens du texte.\n",
        "\n",
        "Tout comme nous l'avons vu dans la section ci-dessus, des mots comme there , book et table ajoutent plus de sens au texte que les mots is et on ."
      ],
      "metadata": {
        "id": "buG6Ly3u87FF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Voici quelques avantages clés de la suppression des mots vides :\n",
        "\n",
        "Lors de la suppression des mots vides, la taille de l'ensemble de données diminue et le temps de formation du modèle diminue également\n",
        "La suppression des mots vides peut potentiellement contribuer à améliorer les performances, car il reste moins de jetons significatifs. Ainsi, cela pourrait augmenter la précision de la classification\n",
        "Même les moteurs de recherche comme Google suppriment les mots vides pour une récupération rapide et pertinente des données de la base de données."
      ],
      "metadata": {
        "id": "uY5GFwuJ899H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Quand devrions-nous supprimer les mots vides ?\n",
        "J'ai résumé cela en deux parties : quand pouvons-nous supprimer les mots vides et quand devons-nous éviter de le faire.\n",
        "\n",
        "**Supprimer les mots vides**\n",
        "\n",
        "Nous pouvons supprimer les mots vides lors de l'exécution des tâches suivantes :\n",
        "\n",
        "Classement du texte\n",
        "\n",
        "Filtrage anti-spam\n",
        "\n",
        "Classification linguistique\n",
        "\n",
        "Classement des genres\n",
        "\n",
        "Génération de sous-titres\n",
        "\n",
        "Génération de balises automatiques\n",
        "\n",
        "**Évitez la suppression des mots vides**\n",
        "\n",
        "Traduction automatique\n",
        "\n",
        "Modélisation du langage\n",
        "\n",
        "Résumé du texte\n",
        "\n",
        "Problèmes de questions-réponses"
      ],
      "metadata": {
        "id": "GIzklv-Y9TFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Différentes méthodes pour supprimer les mots vides\n",
        "\n",
        "1. Suppression des mots vides à l'aide de NLTK\n",
        "\n",
        "NLTK, ou Natural Language Toolkit, est un trésor de bibliothèque pour le prétraitement de texte. C'est l'une de mes bibliothèques Python préférées. NLTK possède une liste de mots vides stockés dans 16 langues différentes."
      ],
      "metadata": {
        "id": "W108sJiyBkWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Vous pouvez utiliser le code ci-dessous pour voir la liste des mots vides dans NLTK :\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v1unOE6Byw9",
        "outputId": "a21fb458-0deb-4bde-e8ea-c1774947cfe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Désormais, pour supprimer les mots vides à l'aide de NLTK, vous pouvez utiliser le bloc de code suivant. Il s'agit d'une fenêtre de codage LIVE afin que vous puissiez jouer avec le code et voir les résultats sans quitter l'article !"
      ],
      "metadata": {
        "id": "iihFfHhaFSef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following code is to remove stop words from sentence using nltk\n",
        "# Created by - ANALYTICS VIDHYA\n",
        "\n",
        "# importing libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "# sample sentence\n",
        "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and\n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had\n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
        "\n",
        "# set of stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# tokens of words\n",
        "word_tokens = word_tokenize(text)\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\\nOriginal Sentence \\n\\n\")\n",
        "print(\" \".join(word_tokens))\n",
        "\n",
        "print(\"\\n\\nFiltered Sentence \\n\\n\")\n",
        "print(\" \".join(filtered_sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QqkHKwcFTF0",
        "outputId": "b38c06ea-f2c4-4afa-c32d-1cec11e47087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Original Sentence \n",
            "\n",
            "\n",
            "He determined to drop his litigation with the monastry , and relinguish his claims to the wood-cuting and fishery rihgts at once . He was the more ready to do this becuase the rights had become much less valuable , and he had indeed the vaguest idea where the wood and river in question were .\n",
            "\n",
            "\n",
            "Filtered Sentence \n",
            "\n",
            "\n",
            "He determined drop litigation monastry , relinguish claims wood-cuting fishery rihgts . He ready becuase rights become much less valuable , indeed vaguest idea wood river question .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici la liste que nous avons obtenue après tokenisation :"
      ],
      "metadata": {
        "id": "KznQAn7LJBzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "He determined to drop his litigation with the monastry, and relinguish his claims to the\n",
        "wood-cuting and fishery rihgts at once. He was the more ready to do this becuase the rights\n",
        "had become much less valuable, and he had indeed the vaguest idea where the wood and river\n",
        " in question were."
      ],
      "metadata": {
        "id": "mNVXfX-vJCbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Et la liste après suppression des mots vides :"
      ],
      "metadata": {
        "id": "1CY3_1ofJDg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "He determined drop litigation monastry, relinguish claims wood-cuting fishery rihgts. He\n",
        "ready becuase rights become much less valuable, indeed vaguest idea wood river question."
      ],
      "metadata": {
        "id": "su7NkzFZJEea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Suppression des mots vides à l'aide de spaCy\n",
        "\n",
        "Voici comment supprimer les mots vides à l’aide de spaCy en Python :"
      ],
      "metadata": {
        "id": "xpc5VLzhJt4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy est l'une des bibliothèques les plus polyvalentes et les plus utilisées en PNL. Nous pouvons supprimer rapidement et efficacement les mots vides du texte donné en utilisant SpaCy. Il possède une liste de ses propres mots vides qui peuvent être importés en tant que STOP_WORDS à partir de la classe spacy.lang.en.stop_words .\n"
      ],
      "metadata": {
        "id": "kuL0iwiJJ5FU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and\n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had\n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
        "\n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "my_doc = nlp(text)\n",
        "\n",
        "# Create list of word tokens\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "    token_list.append(token.text)\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# Create list of word tokens after removing stopwords\n",
        "filtered_sentence =[]\n",
        "\n",
        "for word in token_list:\n",
        "    lexeme = nlp.vocab[word]\n",
        "    if lexeme.is_stop == False:\n",
        "        filtered_sentence.append(word)\n",
        "print(token_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rf1raf90GKhs",
        "outputId": "e95e6396-60b7-40bb-f78c-66aec4c91c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['He', 'determined', 'to', 'drop', 'his', 'litigation', 'with', 'the', 'monastry', ',', 'and', 'relinguish', 'his', 'claims', 'to', 'the', 'wood', '-', 'cuting', 'and', '\\n', 'fishery', 'rihgts', 'at', 'once', '.', 'He', 'was', 'the', 'more', 'ready', 'to', 'do', 'this', 'becuase', 'the', 'rights', 'had', 'become', 'much', 'less', 'valuable', ',', 'and', 'he', 'had', '\\n', 'indeed', 'the', 'vaguest', 'idea', 'where', 'the', 'wood', 'and', 'river', 'in', 'question', 'were', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici la liste que nous avons obtenue après tokenisation :"
      ],
      "metadata": {
        "id": "MF9GahFsH0pU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "He determined to drop his litigation with the monastry and relinguish his claims to the\n",
        "wood-cuting and \\n fishery rihgts at once. He was the more ready to do this becuase the\n",
        "rights had become much less valuable, and he had \\n indeed the vaguest idea where the wood\n",
        " and river in question were."
      ],
      "metadata": {
        "id": "nYa2QH6gI-XU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Et la liste après suppression des mots vides :"
      ],
      "metadata": {
        "id": "pMGhsHe5Izqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "determined drop litigation monastry, relinguish claims wood-cuting \\n fishery rihgts. ready\n",
        "becuase rights become valuable, \\n vaguest idea wood river question."
      ],
      "metadata": {
        "id": "PMeWB3mXI_j4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un point important à noter : la suppression des mots vides n'enlève pas les signes de ponctuation ou les caractères de nouvelle ligne. Nous devrons les supprimer manuellement."
      ],
      "metadata": {
        "id": "orN-oiw-KRND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Suppression des mots vides à l'aide de Gensim"
      ],
      "metadata": {
        "id": "ou37pfv7KXXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gensim est une bibliothèque très pratique avec laquelle travailler sur des tâches PNL. Lors du prétraitement, gensim fournit également des méthodes pour supprimer les mots vides. Nous pouvons facilement importer la méthode remove_stopwords depuis la classe gensim.parsing.preprocessing.\n",
        "\n"
      ],
      "metadata": {
        "id": "hyK-RZLCKk7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following code is to remove stop words using gensim\n",
        "# Created by - ANALYTICS VIDHYA\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "# pass the sentence in the remove_stopwords function\n",
        "result = remove_stopwords(\"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable,\n",
        "and he had indeed the vaguest idea where the wood and river in question were.\"\"\")\n",
        "\n",
        "print('\\n\\n Filtered Sentence \\n\\n')\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSND5kAeH1MP",
        "outputId": "2c041bc4-65f6-47fd-fff8-24d813dc22c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " Filtered Sentence \n",
            "\n",
            "\n",
            "He determined drop litigation monastry, relinguish claims wood-cuting fishery rihgts once. He ready becuase rights valuable, vaguest idea wood river question were.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "resultat:\n",
        "\n",
        "He determined drop litigation monastry, relinguish claims wood-cuting fishery rihgts once.\n",
        "He ready becuase rights valuable, vaguest idea wood river question were."
      ],
      "metadata": {
        "id": "LFRHUQ7ZKum5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tout en utilisant gensim pour supprimer les mots vides, nous pouvons l'utiliser directement sur le texte brut. Il n'est pas nécessaire d'effectuer une tokenisation avant de supprimer les mots vides. Cela peut nous faire gagner beaucoup de temps."
      ],
      "metadata": {
        "id": "SdoaTxbmK_v4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction à la normalisation du texte"
      ],
      "metadata": {
        "id": "6w8JjD60bD-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Que sont le stemming et la lemmatisation ?\n",
        "**La radicalisation et la lemmatisation** sont simplement une normalisation des mots, ce qui signifie réduire un mot à sa forme racine."
      ],
      "metadata": {
        "id": "cC4YXJ_8bUC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dérivé\n",
        "Comprenons d'abord la racine :\n",
        "\n",
        "**Le stemming** est une technique de normalisation de texte qui coupe la fin ou le début d'un mot en prenant en compte une liste de préfixes ou suffixes courants qui pourraient être trouvés dans ce mot.\n",
        "Il s'agit d'un processus rudimentaire basé sur des règles consistant à supprimer les suffixes (« ing », « ly », « es », « s », etc.) d'un mot."
      ],
      "metadata": {
        "id": "TDGCExWlbafG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lemmatisation\n",
        "La lemmatisation, quant à elle, est une procédure organisée et étape par étape pour obtenir la forme racine du mot. Il fait appel au vocabulaire (importance des mots dans le dictionnaire) et à l'analyse morphologique (structure des mots et relations grammaticales)."
      ],
      "metadata": {
        "id": "0DqhMlrbbngL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pourquoi devons-nous effectuer un stemming ou une lemmatisation ?\n",
        "Considérons les deux phrases suivantes :\n",
        "\n",
        "Il **conduisait**\n",
        "\n",
        "Il est **allé faire** un tour\n",
        "\n",
        "Nous pouvons facilement affirmer que les deux phrases véhiculent le même sens, c’est-à-dire conduire une activité dans le passé. Une machine traitera les deux phrases différemment. Ainsi, pour rendre le texte compréhensible pour la machine, nous devons effectuer une radicalisation ou une lemmatisation.\n",
        "\n",
        "Un autre avantage de la normalisation du texte est qu'elle réduit le nombre de mots uniques dans les données textuelles. Cela contribue à réduire le temps de formation du modèle d'apprentissage automatique (et ne le voulons-nous pas tous ?).\n",
        "\n",
        "##S0, lequel faut-il préférer ?\n",
        "L'algorithme de recherche de radicaux fonctionne en supprimant le suffixe ou le préfixe du mot. La lemmatisation est une opération plus puissante car elle prend en compte l'analyse morphologique du mot.\n",
        "\n",
        "La lemmatisation renvoie le lemme, qui est la racine de toutes ses formes flexionnelles.\n",
        "\n",
        "Nous pouvons dire que la recherche de racines est une méthode rapide et sale pour couper des mots jusqu'à leur forme racine, tandis que d'un autre côté, la lemmatisation est une opération intelligente qui utilise des dictionnaires créés par des connaissances linguistiques approfondies. Par conséquent, la lemmatisation aide à former de meilleures fonctionnalités."
      ],
      "metadata": {
        "id": "dP2Dytjybn3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Normalisation du texte à l'aide de NLTK\n",
        "La bibliothèque NLTK propose de nombreuses méthodes étonnantes pour effectuer différentes étapes de prétraitement des données. Il existe des méthodes telles que **PorterStemmer()** et **WordNetLemmatizer()** pour effectuer respectivement la radicalisation et la lemmatisation.\n",
        "\n",
        "Voyons-les en action."
      ],
      "metadata": {
        "id": "2iiVoTC7cp2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "set(stopwords.words('english'))\n",
        "\n",
        "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and\n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had\n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_tokens = word_tokenize(text)\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "\n",
        "Stem_words = []\n",
        "ps =PorterStemmer()\n",
        "for w in filtered_sentence:\n",
        "    rootWord=ps.stem(w)\n",
        "    Stem_words.append(rootWord)\n",
        "print(filtered_sentence)\n",
        "print(Stem_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c53ISrs2dBPC",
        "outputId": "641e6cf8-2077-4cfc-b6f3-0d4087572b4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['He', 'determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood-cuting', 'fishery', 'rihgts', '.', 'He', 'ready', 'becuase', 'rights', 'become', 'much', 'less', 'valuable', ',', 'indeed', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n",
            "['he', 'determin', 'drop', 'litig', 'monastri', ',', 'relinguish', 'claim', 'wood-cut', 'fisheri', 'rihgt', '.', 'he', 'readi', 'becuas', 'right', 'becom', 'much', 'less', 'valuabl', ',', 'inde', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On voit clairement la différence ici. Maintenant, effectuons une lemmatisation sur le même texte."
      ],
      "metadata": {
        "id": "3HXkCnQqfX_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "set(stopwords.words('english'))\n",
        "\n",
        "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and\n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had\n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_tokens = word_tokenize(text)\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "print(filtered_sentence)\n",
        "\n",
        "lemma_word = []\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "for w in filtered_sentence:\n",
        "    word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\")\n",
        "    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
        "    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
        "    lemma_word.append(word3)\n",
        "print(lemma_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKvE-r2wdIS6",
        "outputId": "e917e803-0b4f-43c9-b57c-b09d3f4caff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['He', 'determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood-cuting', 'fishery', 'rihgts', '.', 'He', 'ready', 'becuase', 'rights', 'become', 'much', 'less', 'valuable', ',', 'indeed', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n",
            "['He', 'determine', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claim', 'wood-cuting', 'fishery', 'rihgts', '.', 'He', 'ready', 'becuase', 'right', 'become', 'much', 'le', 'valuable', ',', 'indeed', 'vague', 'idea', 'wood', 'river', 'question', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici, v représente le verbe , a représente l'adjectif et n représente le nom . Le lemmatiseur ne lemmatise que les mots qui correspondent au paramètre pos de la méthode lemmatize.\n",
        "\n",
        "La lemmatisation est réalisée sur la base d'un marquage de partie du discours (étiquetage POS). Nous parlerons en détail du marquage POS dans un prochain article."
      ],
      "metadata": {
        "id": "bllaD6oBgMiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Normalisation du texte à l'aide de spaCy\n",
        "spaCy, comme nous l'avons vu plus tôt, est une étonnante bibliothèque PNL. Il fournit de nombreuses méthodes industrielles pour effectuer la lemmatisation. Malheureusement, spaCy n'a pas de module de stemming. Pour effectuer la lemmatisation, consultez le code ci-dessous :"
      ],
      "metadata": {
        "id": "60tp8au1gTKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make sure to download the english model with \"python -m spacy download en\"\n",
        "\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "doc = nlp(u\"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and\n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had\n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\")\n",
        "\n",
        "lemma_word1 = []\n",
        "for token in doc:\n",
        "    lemma_word1.append(token.lemma_)\n",
        "lemma_word1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey09VBGmgZEt",
        "outputId": "23ad9bbc-8f9d-4f5e-b3fe-64050e7fe50a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he',\n",
              " 'determine',\n",
              " 'to',\n",
              " 'drop',\n",
              " 'his',\n",
              " 'litigation',\n",
              " 'with',\n",
              " 'the',\n",
              " 'monastry',\n",
              " ',',\n",
              " 'and',\n",
              " 'relinguish',\n",
              " 'his',\n",
              " 'claim',\n",
              " 'to',\n",
              " 'the',\n",
              " 'wood',\n",
              " '-',\n",
              " 'cut',\n",
              " 'and',\n",
              " '\\n',\n",
              " 'fishery',\n",
              " 'rihgts',\n",
              " 'at',\n",
              " 'once',\n",
              " '.',\n",
              " 'he',\n",
              " 'be',\n",
              " 'the',\n",
              " 'more',\n",
              " 'ready',\n",
              " 'to',\n",
              " 'do',\n",
              " 'this',\n",
              " 'becuase',\n",
              " 'the',\n",
              " 'right',\n",
              " 'have',\n",
              " 'become',\n",
              " 'much',\n",
              " 'less',\n",
              " 'valuable',\n",
              " ',',\n",
              " 'and',\n",
              " 'he',\n",
              " 'have',\n",
              " '\\n',\n",
              " 'indeed',\n",
              " 'the',\n",
              " 'vague',\n",
              " 'idea',\n",
              " 'where',\n",
              " 'the',\n",
              " 'wood',\n",
              " 'and',\n",
              " 'river',\n",
              " 'in',\n",
              " 'question',\n",
              " 'be',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-PRON- décide d'abandonner le litige -PRON- avec le monastère et d'abandonner la réclamation -PRON-\n",
        "\n",
        "aux droits de coupe de bois et de pêche à la fois. -PRON- soyez le plus prêt à faire ça\n",
        "parce que les droits ont beaucoup moins de valeur, et -PRON- ont effectivement \\n la vague idée\n",
        "où se trouvent le bois et la rivière en question.\n",
        "\n",
        "Ici -PRON- est la notation du pronom qui pourrait facilement être supprimée à l'aide d'expressions régulières. L'avantage de spaCy est que nous n'avons besoin de passer aucun paramètre pos pour effectuer la lemmatisation."
      ],
      "metadata": {
        "id": "1VDaDA5HglcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Normalisation du texte à l'aide de TextBlob\n",
        "TextBlob est une bibliothèque Python spécialement conçue pour le prétraitement des données texte. Il est basé sur la bibliothèque NLTK . Nous pouvons utiliser TextBlob pour effectuer la lemmatisation. Cependant, il n'y a pas de module de recherche de racines dans TextBlob."
      ],
      "metadata": {
        "id": "MD_AGLjkguAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from textblob lib import Word method\n",
        "from textblob import Word\n",
        "\n",
        "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and\n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had\n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
        "\n",
        "lem = []\n",
        "for i in text.split():\n",
        "    word1 = Word(i).lemmatize(\"n\")\n",
        "    word2 = Word(word1).lemmatize(\"v\")\n",
        "    word3 = Word(word2).lemmatize(\"a\")\n",
        "    lem.append(Word(word3).lemmatize())\n",
        "print(lem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75St2YXog1gG",
        "outputId": "339e851a-94bb-4ee7-93c7-7862af1ed31e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['He', 'determine', 'to', 'drop', 'his', 'litigation', 'with', 'the', 'monastry,', 'and', 'relinguish', 'his', 'claim', 'to', 'the', 'wood-cuting', 'and', 'fishery', 'rihgts', 'at', 'once.', 'He', 'wa', 'the', 'more', 'ready', 'to', 'do', 'this', 'becuase', 'the', 'right', 'have', 'become', 'much', 'le', 'valuable,', 'and', 'he', 'have', 'indeed', 'the', 'vague', 'idea', 'where', 'the', 'wood', 'and', 'river', 'in', 'question', 'were.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il décide d'abandonner son litige avec le monastère et de renoncer à ses prétentions au\n",
        "droits de coupe de bois et de pêche à la fois. Il était d'autant plus disposé à le faire que le droit\n",
        "sont devenus beaucoup moins précieux, et il a en effet la vague idée où se trouvent le bois et la rivière.\n",
        "en question étaient.\n",
        "Tout comme nous l'avons vu ci-dessus dans la section NLTK, TextBlob utilise également le balisage POS pour effectuer la lemmatisation. Vous pouvez en savoir plus sur l'utilisation de TextBlob en PNL ici :"
      ],
      "metadata": {
        "id": "0XEiQVbPg_BI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming vs Lemmatisation en PNL : différences à connaître"
      ],
      "metadata": {
        "id": "PwhMafTIg6B5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Qu’est-ce que le stemming en PNL ?**\n",
        "\n",
        "C'est le processus de réduction des mots infectés à leur racine. Par exemple, dans la figure 1, remplacez les mots « histoire » et « historique » par « histori ». De même pour les mots enfin et final.\n",
        "\n",
        "La radicalisation est le processus qui consiste à supprimer les derniers caractères d'un mot donné, pour obtenir une forme plus courte, même si cette forme n'a aucune signification."
      ],
      "metadata": {
        "id": "vtEuqqn5jFaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pourquoi avons-nous besoin d’un stemming ?**\n",
        "\n",
        "Dans les cas d'utilisation de la PNL tels que l'analyse des sentiments, la classification du spam, les critiques de restaurants, etc., il est important d'obtenir le mot de base pour savoir si le mot est positif ou négatif. La radicalisation est utilisée pour obtenir ce mot de base."
      ],
      "metadata": {
        "id": "ChRkg3-LjNxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importer des bibliothèques\n",
        "#Importez les bibliothèques qui seront nécessaires pour le stemming.\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiTtg3kzjVux",
        "outputId": "10405b65-16aa-4a3b-a430-c28c98cfcc89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtenez l'entrée\n",
        "#Le paragraphe sera pris comme entrée et utilisé pour la racine.\n",
        "\n",
        "paragraph = \"\"\"\n",
        "    I have three visions for India. In 3000 years of our history,\n",
        "    people from all over the world have come and invaded us, captured our  lands, conquered our minds.\n",
        "    From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "    the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "    Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "    We have not grabbed their land, their culture,\n",
        "    their history and tried to enforce our way of life on them.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "x4B2-oENjhwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenisation (étape avant la tige)**\n",
        "\n",
        "Avant, la tokenisation est effectuée de manière à diviser le texte en morceaux. Dans ce cas, paragraphe en phrases pour un calcul facile.\n",
        "\n",
        "Comme le montre le paragraphe de sortie, il est divisé en phrases basées sur « ». .\n",
        "\n",
        "**Dérivé**\n",
        "\n",
        "Dans le code ci-dessous, une phrase est prise à la fois et la tokenisation des mots est appliquée, c'est-à-dire la conversion de la phrase en mots. Après cela, les mots vides (tels que le, et, etc.) sont ignorés et la radicalisation est appliquée à tous les autres mots. Enfin, les mots radicaux sont joints pour former une phrase.\n",
        "\n",
        "Remarque : Les mots vides sont les mots qui n’ajoutent aucune valeur à la phrase.\n",
        "\n",
        "**Code Python :**"
      ],
      "metadata": {
        "id": "QL2gMBkUj8i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over  the world have come and invaded us, captured our lands, conquered our minds.  From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours.  Yet we have not done this to any other nation. We have not conquered anyone. We have not grabbed their land, their culture, their history and tried to enforce our way of life on them.  \"\"\"\n",
        "\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "print(sentences)\n",
        "\n",
        "\n",
        "print(\"\\n\\n Result after Stemming \\n\\n\")\n",
        "stemmer = nltk.PorterStemmer()\n",
        "# Stemming\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)\n",
        "\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZfP4wrUkKaj",
        "outputId": "76e8523b-6bb4-43d8-ff83-05cd0a465068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I have three visions for India.', 'In 3000 years of our history, people from all over  the world have come and invaded us, captured our lands, conquered our minds.', 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours.', 'Yet we have not done this to any other nation.', 'We have not conquered anyone.', 'We have not grabbed their land, their culture, their history and tried to enforce our way of life on them.']\n",
            "\n",
            "\n",
            " Result after Stemming \n",
            "\n",
            "\n",
            "['i three vision india .', 'in 3000 year histori , peopl world come invad us , captur land , conquer mind .', 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .', 'yet done nation .', 'we conquer anyon .', 'we grab land , cultur , histori tri enforc way life .']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "À partir du résultat ci-dessus, nous pouvons voir que les mots vides tels que have, for ont été supprimés de la première phrase. Le mot « visions » a été converti en « vision », « histoire » en « histori » par racine."
      ],
      "metadata": {
        "id": "UChjo125kic5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Qu’est-ce que la lemmatisation en PNL ?**\n",
        "\n",
        "Le but de la lemmatisation est le même que celui du stemming mais surmonte les inconvénients du stemming. Dans la racine, pour certains mots, cela peut ne pas donner une représentation significative comme « Histori ». Ici, la lemmatisation entre en scène car elle donne un mot significatif.\n",
        "\n",
        "La lemmatisation prend plus de temps que la recherche de racines, car elle trouve un mot/une représentation significative. La recherche de racines nécessite simplement d'obtenir un mot de base et prend donc moins de temps.\n",
        "\n",
        "Le stemming a son application dans l'analyse des sentiments tandis que la lemmatisation a son application dans les chatbots, la réponse humaine.\n",
        "\n",
        "**Code de lemmatisation expliqué**\n",
        "\n",
        "Dans le même ordre d'idées, nous importerons des bibliothèques pour obtenir des entrées pour la lemmatisation.\n",
        "\n",
        "**Importer des bibliothèques**"
      ],
      "metadata": {
        "id": "_VD4boYclDHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACOibroZlMZK",
        "outputId": "fb5653e7-a542-4fe2-e054-fe9f4b3940f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtenez l'entrée\n",
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
        "               the world have come and invaded us, captured our lands, conquered our minds.\n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "               We have not grabbed their land, their culture,\n",
        "               their history and tried to enforce our way of life on them.\n",
        "               \"\"\""
      ],
      "metadata": {
        "id": "yvpZI1v_ld6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenisation (étape avant la tige)\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0EtkhEClgjS",
        "outputId": "6cc72611-5d78-4718-e769-76e1adcacc9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I have three visions for India.', 'In 3000 years of our history, people from all over \\n               the world have come and invaded us, captured our lands, conquered our minds.', 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.', 'Yet we have not done this to any other nation.', 'We have not conquered anyone.', 'We have not grabbed their land, their culture, \\n               their history and tried to enforce our way of life on them.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatisation**\n",
        "\n",
        "La différence entre la radicalisation et la lemmatisation apparaît dans cette étape où WordNetLemmatizer() est utilisé à la place de PorterStemmer(). Le reste des étapes est le même."
      ],
      "metadata": {
        "id": "GR4e5Q1-m9as"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "# Lemmatization\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)\n",
        "#Obtenez le résultat\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIN9bsVrnEOY",
        "outputId": "08153f1a-2849-489e-8d8e-dda09177164e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I three vision India .', 'In 3000 year history , people world come invaded u , captured land , conquered mind .', 'From Alexander onwards , Greeks , Turks , Moguls , Portuguese , British , French , Dutch , came looted u , took .', 'Yet done nation .', 'We conquered anyone .', 'We grabbed land , culture , history tried enforce way life .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sortir:**\n",
        "\n",
        "Dans le résultat ci-dessus, on peut remarquer que bien que le mot « visions » ait été converti en « vision », le mot « histoire » est resté « histoire » contrairement au radical et a donc conservé sa signification."
      ],
      "metadata": {
        "id": "k5pDTXDLnb-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Racine vs Lemmatisation**\n",
        "\n",
        "**Dérivé**\n",
        "\n",
        "La recherche de racines est un processus qui consiste à extraire ou à supprimer les derniers caractères d'un mot, ce qui conduit souvent à des significations et à une orthographe incorrectes.\n",
        "Par exemple, dériver le mot « Caring » renverrait « Car ».\n",
        "La recherche de racines est utilisée dans le cas d'un ensemble de données volumineux où les performances sont un problème.\n",
        "\n",
        "**Lemmatisation**\n",
        "\n",
        "La lemmatisation prend en compte le contexte et convertit le mot en sa forme de base significative, appelée Lemme.\n",
        "Par exemple, lemmatiser le mot « Caring » renverrait « Care ».\n",
        "La lemmatisation est coûteuse en termes de calcul car elle implique des tables de recherche et ainsi de suite."
      ],
      "metadata": {
        "id": "4t0TwLPOoHP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comment fonctionne le stemming ?**\n",
        "\n",
        "La recherche de racines est un processus de normalisation linguistique dans le traitement du langage naturel et la recherche d'informations. Son objectif principal est de réduire les mots à leur forme de base ou racine, connue sous le nom de radical. La radicalisation permet de regrouper des mots ayant des significations ou des racines similaires, même s'ils ont des inflexions, des préfixes ou des suffixes différents.\n",
        "\n",
        "Le processus consiste à supprimer les affixes courants (préfixes, suffixes) des mots, ce qui donne lieu à une forme simplifiée qui représente la signification fondamentale du mot. La recherche de racines est un processus heuristique et ne peut produire qu'occasionnellement un mot valide. Néanmoins, il est efficace pour des tâches telles que la recherche d’informations, où l’accent est mis sur la correspondance du sens essentiel des mots plutôt que sur leur exactitude grammaticale.\n",
        "\n",
        "Par exemple:\n",
        "\n",
        "Exécution -> Exécuter\n",
        "\n",
        "Sauts -> Sauter\n",
        "\n",
        "Natation -> Nager\n",
        "\n",
        "Les algorithmes de recherche de racines utilisent diverses règles et heuristiques pour identifier et supprimer les affixes, ce qui les rend largement applicables dans les tâches de traitement de texte afin d'améliorer la récupération et l'analyse d'informations.\n",
        "\n",
        "**Comment fonctionne la lemmatisation ?**\n",
        "\n",
        "La lemmatisation est un processus linguistique qui consiste à réduire les mots à leur forme de base ou racine, connue sous le nom de lemme. Le but est de normaliser les différentes formes fléchies d’un mot afin qu’elles puissent être analysées ou comparées plus facilement. Ceci est particulièrement utile dans le traitement du langage naturel (NLP) et l’analyse de texte.\n",
        "\n",
        "Voici comment fonctionne généralement la lemmatisation :\n",
        "\n",
        "**Tokenisation:** la première étape consiste à décomposer un texte en mots ou jetons individuels. Cela peut être fait en utilisant diverses méthodes, telles que diviser le texte en fonction des espaces.\n",
        "\n",
        "**Marquage POS :** le balisage de parties du discours implique l'attribution d'une catégorie grammaticale (comme un nom, un verbe, un adjectif, etc.) à chaque jeton. La lemmatisation repose souvent sur ces informations, car la forme de base d'un mot peut dépendre de son rôle grammatical dans une phrase.\n",
        "\n",
        "Lemmatisation : une fois que chaque mot a été tokenisé et attribué une balise de partie du discours, l'algorithme de lemmatisation utilise un lexique ou des règles linguistiques pour déterminer le lemme de chaque mot. Le lemme est la forme de base du mot, qui n'est pas nécessairement la même que la racine du mot. Par exemple, le lemme de « courir » est « courir » et le lemme de « mieux » (dans le contexte d’un adjectif) est « bon ».\n",
        "\n",
        "**Application des règles:** les algorithmes de lemmatisation s'appuient souvent sur des règles et des modèles linguistiques. Pour les verbes irréguliers ou les mots avec plusieurs lemmes possibles, ces règles aident à prendre la bonne décision de lemmatisation.\n",
        "\n",
        "**Résultat:** Le résultat de la lemmatisation est un ensemble de mots sous leur forme de base ou de dictionnaire, ce qui facilite l'analyse et la compréhension du sens sous-jacent d'un texte.\n",
        "\n",
        "La lemmatisation est distincte du stemming, une autre technique de normalisation de texte. Alors que la recherche de racines consiste à couper les préfixes ou les suffixes des mots pour obtenir une racine commune, la lemmatisation vise à obtenir une forme de base valide grâce à l'analyse linguistique. La lemmatisation a tendance à être plus précise mais peut être plus coûteuse en termes de calcul que la recherche de racines."
      ],
      "metadata": {
        "id": "Utiz3X-9o64K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "\n",
        "Une chose à noter est que la lemmatisation nécessite beaucoup de connaissances et de compréhension de la structure du langage. Par conséquent, dans toute nouvelle langue, créer un stemmer est plus facile qu’un algorithme de lemmatisation. Lorsque l’on considère la recherche de racines par rapport à la lemmatisation, il devient évident que la recherche de racines se concentre sur la suppression des préfixes et des suffixes pour obtenir des racines de mots, ce qui en fait un processus plus simple, tandis que la lemmatisation implique la compréhension de la forme racine des mots, exigeant une compréhension linguistique plus profonde.\n",
        "\n",
        "\n",
        "La lemmatisation et la radicalisation sont à la base des mots dérivés (infléchis) et donc la seule différence entre le lemme et le radical est que le lemme est un mot réel alors que le radical peut ne pas être un véritable mot de langage.\n",
        "\n",
        "La lemmatisation utilise un corpus pour atteindre un lemme, ce qui la rend plus lente que le stemming. De plus, pour obtenir le lemme approprié, vous devrez peut-être définir des parties du discours. Alors que, pour extraire un algorithme par étapes, il est suivi, ce qui le rend plus rapide.\n",
        "\n",
        "Les points ci-dessus montrent que le stemming doit être utilisé si la vitesse est importante puisque les lemmatiseurs analysent un corpus, ce qui est une tâche qui prend du temps. De plus, le choix entre les lemmatiseurs et les stemmers dépend également du problème sur lequel vous travaillez."
      ],
      "metadata": {
        "id": "aOxAP3ETqs0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions fréquemment posées\n",
        "\n",
        "**T1. Quelle est la meilleure lemmatisation ou stemming ?**\n",
        "\n",
        "R. Le choix dépend du cas d’utilisation spécifique. La lemmatisation produit un mot linguistiquement valide tandis que la recherche de racines est plus rapide mais peut générer des non-mots.\n",
        "\n",
        "**Q2. Faites-vous à la fois du stemming et de la lemmatisation ?**\n",
        "\n",
        "A. En tant que modèle de langage d'IA, je peux effectuer à la fois la recherche de racines et la lemmatisation en fonction des exigences ou du contexte de la tâche.\n",
        "\n",
        "**Q3. Pourquoi la tige est-elle plus rapide que la lemmatisation ?**\n",
        "\n",
        "A. La radicalisation coupe les terminaisons des mots sans tenir compte du contexte linguistique, ce qui rend le calcul plus rapide. La lemmatisation analyse les formes des mots pour déterminer la forme de base ou de dictionnaire, ce qui prend plus de temps de traitement.\n",
        "\n",
        "**Q4. Quelle est l’application du stemming et de la lemmatisation ?**\n",
        "\n",
        "A. La recherche de radicaux et la lemmatisation sont utilisées dans les tâches de traitement du langage naturel telles que la recherche d'informations, l'exploration de texte, l'analyse des sentiments et les moteurs de recherche pour réduire les mots à leur forme de base ou racine pour une meilleure analyse et compréhension."
      ],
      "metadata": {
        "id": "JqE04tX3r_KW"
      }
    }
  ]
}