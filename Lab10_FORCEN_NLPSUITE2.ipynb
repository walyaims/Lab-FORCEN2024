{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!cd python-package/\n",
        "!pip install -v ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ahreSontLLN",
        "outputId": "a35fe4c6-a114-403b-d1c2-13628af158ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: cd: python-package/: No such file or directory\n",
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "print(keras.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeusAqJlumJz",
        "outputId": "829ffb2d-09ff-45ab-8d18-a2e35e268f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "import keras"
      ],
      "metadata": {
        "id": "TBuCem7Ay7ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Un guide complet pour comprendre et implémenter la classification de texte en Python"
      ],
      "metadata": {
        "id": "iICxHGFefwmW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYNqoHAbfSDN"
      },
      "outputs": [],
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "import pandas, xgboost, numpy, textblob, string"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Préparation de l'ensemble de données\n",
        "\n",
        "Pour les besoins de cet article, j'utilise un ensemble de données d'avis Amazon qui peuvent être téléchargés sur ce lien . L'ensemble de données se compose de 3,6 millions de critiques de textes et de leurs étiquettes, nous n'utiliserons qu'une petite fraction des données. Pour préparer l'ensemble de données, chargez les données téléchargées dans un dataframe pandas contenant deux colonnes : texte et étiquette. ( Source )"
      ],
      "metadata": {
        "id": "y-06-c-KhOQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "data = open('/content/corpus').read()\n",
        "labels, texts = [], []\n",
        "for i, line in enumerate(data.split(\"\\n\")):\n",
        "    content = line.split()\n",
        "    labels.append(content[0])\n",
        "    texts.append(\" \".join(content[1:]))\n",
        "\n",
        "# create a dataframe using texts and lables\n",
        "trainDF = pandas.DataFrame()\n",
        "trainDF['text'] = texts\n",
        "trainDF['label'] = labels"
      ],
      "metadata": {
        "id": "J_1ft21iiYZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensuite, nous diviserons l'ensemble de données en ensembles de formation et de validation afin de pouvoir former et tester le classificateur. Nous coderons également notre colonne cible afin qu’elle puisse être utilisée dans des modèles d’apprentissage automatique."
      ],
      "metadata": {
        "id": "kDjPDtN9jJvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset into training and validation datasets\n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
        "\n",
        "# label encode the target variable\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "metadata": {
        "id": "Mnopi-vfjK5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "zWuwJ9pbcqQY",
        "outputId": "bc40f328-6d52-48ef-87ee-21d90c8ff2e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGWdha_9ckRy",
        "outputId": "bf1a8ecd-7b92-4737-a09c-5e125cab764d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY58GVlocns0",
        "outputId": "3235d494-9c79-49d2-a24f-2d54d2256cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Ingénierie des fonctionnalités\n",
        "La prochaine étape est l’étape d’ingénierie des fonctionnalités. Au cours de cette étape, les données textuelles brutes seront transformées en vecteurs de caractéristiques et de nouvelles fonctionnalités seront créées à l'aide de l'ensemble de données existant. Nous mettrons en œuvre les différentes idées suivantes afin d'obtenir des fonctionnalités pertinentes à partir de notre ensemble de données.\n",
        "\n",
        "2.1 Compter les vecteurs comme caractéristiques\n",
        "\n",
        "2.2 Vecteurs TF-IDF comme caractéristiques\n",
        "\n",
        "Niveau de mot\n",
        "\n",
        "Niveau N-Gram\n",
        "\n",
        "Niveau du personnage\n",
        "\n",
        "2.3 Intégrations de mots en tant que fonctionnalités\n",
        "\n",
        "2.4 Fonctionnalités basées sur le texte / PNL\n",
        "\n",
        "2.5 Modèles de sujets en tant que fonctionnalités\n",
        "\n",
        "Examinons en détail la mise en œuvre de ces idées.\n",
        "\n",
        "**2.1 Compter les vecteurs comme caractéristiques**\n",
        "\n",
        "Count Vector est une notation matricielle de l'ensemble de données dans laquelle chaque ligne représente un document du corpus, chaque colonne représente un terme du corpus et chaque cellule représente le nombre de fréquences d'un terme particulier dans un document particulier."
      ],
      "metadata": {
        "id": "Bv4uetwNnYWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a count vectorizer object\n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(trainDF['text'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vect.transform(train_x)\n",
        "xvalid_count =  count_vect.transform(valid_x)"
      ],
      "metadata": {
        "id": "7VcEBln6ntD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTmtG6YrcuBH",
        "outputId": "58cbf20b-50ab-4218-9514-fbc7acb58bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<7500x31666 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 437415 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Vecteurs TF-IDF en tant que caractéristiques**\n",
        "\n",
        "Le score TF-IDF représente l'importance relative d'un terme dans le document et dans l'ensemble du corpus. Le score TF-IDF est composé de deux termes : le premier calcule la fréquence des termes normalisée (TF), le deuxième terme est la fréquence inverse des documents (IDF), calculée comme le logarithme du nombre de documents dans le corpus divisé par le nombre. des documents où le terme spécifique apparaît.\n",
        "\n",
        "TF(t) = (Nombre de fois où le terme t apparaît dans un document) / (Nombre total de termes dans le document)\n",
        "\n",
        "IDF(t) = log_e (Nombre total de documents / Nombre de documents contenant le terme t)\n",
        "\n",
        "Les vecteurs TF-IDF peuvent être générés à différents niveaux de jetons d'entrée (mots, caractères, n-grammes)\n",
        "\n",
        "**a. Word Level TF-IDF:** Matrice représentant les scores tf-idf de chaque terme dans différents documents\n",
        "\n",
        "**b. N-gramme Niveau TF-IDF:**  Les N-grammes sont la combinaison de N termes ensemble. Cette matrice représentant les scores tf-idf de N-grammes\n",
        "\n",
        "**c. Character Level TF-IDF:** Matrice représentant les scores tf-idf des n-grammes de niveau caractère dans le corpus"
      ],
      "metadata": {
        "id": "jWjVyAjUpBzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(trainDF['text'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "\n",
        "# ngram level tf-idf\n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(trainDF['text'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x)\n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_Cb-AOvr07B",
        "outputId": "23e6c179-b36a-48f9-8c19-1b68348cba1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:558: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vect_ngram_chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "GLz9dv0vc9XR",
        "outputId": "4f78b89e-fd8a-4d3b-c65c-6058890f3ac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='char', max_features=5000, ngram_range=(2, 3),\n",
              "                token_pattern='\\\\w{1,}')"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(analyzer=&#x27;char&#x27;, max_features=5000, ngram_range=(2, 3),\n",
              "                token_pattern=&#x27;\\\\w{1,}&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(analyzer=&#x27;char&#x27;, max_features=5000, ngram_range=(2, 3),\n",
              "                token_pattern=&#x27;\\\\w{1,}&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3 Word Embeddings === Intégrations de mots**\n",
        "\n",
        "Une intégration de mots est une forme de représentation de mots et de documents à l'aide d'une représentation vectorielle dense. La position d'un mot dans l'espace vectoriel est apprise à partir du texte et est basée sur les mots qui entourent le mot lorsqu'il est utilisé. Les incorporations de mots peuvent être entraînées à l'aide du corpus d'entrée lui-même ou peuvent être générées à l'aide d'incorporations de mots pré-entraînées telles que Glove, FastText et Word2Vec . N’importe lequel d’entre eux peut être téléchargé et utilisé comme apprentissage par transfert. On peut en savoir plus sur les intégrations de mots  ici.\n",
        "\n",
        "L'extrait suivant montre comment utiliser les intégrations de mots pré-entraînées dans le modèle. Il y a quatre étapes essentielles :\n",
        "\n",
        "Chargement des intégrations de mots pré-entraînées\n",
        "\n",
        "Création d'un objet tokenizer\n",
        "\n",
        "Transformer des documents texte en séquence de jetons et les compléter\n",
        "\n",
        "Créer un mappage des jetons et de leurs intégrations respectives"
      ],
      "metadata": {
        "id": "4XAk9qQ1sQoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the pre-trained word-embedding vectors\n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('data/wiki-news-300d-1M.vec')):\n",
        "    values = line.split()\n",
        "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
        "\n",
        "# create a tokenizer\n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(trainDF['text'])\n",
        "word_index = token.word_index\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors\n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
        "\n",
        "# create token-embedding mapping\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "BojztBwNumh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4 Fonctionnalités basées sur le texte/PNL**\n",
        "\n",
        "Un certain nombre de fonctionnalités supplémentaires basées sur le texte peuvent également être créées, qui sont parfois utiles pour améliorer les modèles de classification de texte. Certains exemples sont:\n",
        "\n",
        "Nombre de mots des documents – nombre total de mots dans les documents\n",
        "Nombre de caractères des documents – nombre total de caractères dans les documents\n",
        "\n",
        "Densité moyenne des mots des documents – longueur moyenne des mots utilisés dans les documents\n",
        "\n",
        "Nombre de ponctuations dans l'essai complet – nombre total de signes de ponctuation dans les documents\n",
        "Nombre de majuscules dans l'essai complet - nombre total de mots en majuscules dans les documents\n",
        "\n",
        "Nombre de mots du titre dans l'essai complet - nombre total de mots en casse (titre) dans les documents\n",
        "\n",
        "Distribution de fréquence des balises de partie du discours :\n",
        "\n",
        "Nombre de noms\n",
        "\n",
        "Nombre de verbes\n",
        "\n",
        "Nombre d'adjectifs\n",
        "\n",
        "Nombre d'adverbes\n",
        "\n",
        "Nombre de pronoms\n",
        "\n",
        "Ces fonctionnalités sont hautement expérimentales et doivent être utilisées uniquement en fonction de l'énoncé du problème."
      ],
      "metadata": {
        "id": "1vvXWiaJu1LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainDF [ 'char_count' ]  =  trainDF [ 'text' ] . apply ( len )\n",
        "trainDF [ 'word_count' ]  =  trainDF [ 'text' ] . appliquer ( lambda  x :  len ( x . split ()))\n",
        "trainDF [ 'word_density' ]  =  trainDF [ 'char_count' ]  /  ( trainDF [ 'word_count' ] + 1 )\n",
        "trainDF [ 'punctuation_count' ]  =  trainDF [ 'text' ] . apply ( lambda  x :  len ( \"\" . join ( _  for  _  in  x  if  _  in  string . ponctuation )))\n",
        "trainDF [ 'title_word_count' ]  =  trainDF [ 'text' ] . apply ( lambda  x :  len ([ wrd  for  wrd  in  x . split ()  if  wrd . istitle ()]))\n",
        "trainDF [ 'upper_case_word_count' ]  =  trainDF [ 'text' ] . appliquer ( lambda  x :  len ([ wrd  for  wrd  in  x . split ()  if  wrd . isupper ()]))"
      ],
      "metadata": {
        "id": "Iy6FZM20vVFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fonction pour vérifier et obtenir le nombre de balises de partie du discours d'un mot dans une phrase donnée\n",
        "def  check_pos_tag ( x ,  flag ) :\n",
        "    cnt  =  0\n",
        "    try :\n",
        "        wiki  =  textblob . TextBlob ( x )\n",
        "        pour  tup  dans  wiki . tags :\n",
        "            ppo  =  list ( tup )[ 1 ]\n",
        "            si  ppo  dans  pos_family [ flag ] :\n",
        "                cnt  +=  1\n",
        "    sauf :\n",
        "        pass\n",
        "    return  cnt"
      ],
      "metadata": {
        "id": "BKYiJjZqvb66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Construction de modèles**\n",
        "\n",
        "La dernière étape du cadre de classification de texte consiste à former un classificateur à l'aide des fonctionnalités créées à l'étape précédente. Il existe de nombreux choix différents de modèles d'apprentissage automatique qui peuvent être utilisés pour entraîner un modèle final. Nous implémenterons les différents classificateurs suivants à cet effet :\n",
        "\n",
        "Classificateur naïf de Bayes\n",
        "\n",
        "Classificateur linéaire\n",
        "\n",
        "Machine à vecteurs de support\n",
        "\n",
        "Modèles d'ensachage\n",
        "\n",
        "Stimuler les modèles\n",
        "\n",
        "Réseaux de neurones peu profonds\n",
        "\n",
        "Réseaux de neurones profonds\n",
        "\n",
        "Réseau neuronal convolutif (CNN)\n",
        "\n",
        "Modélisateur Long Court Terme (LSTM)\n",
        "\n",
        "Unité récurrente fermée (GRU)\n",
        "\n",
        "RNN bidirectionnel\n",
        "\n",
        "Réseau neuronal convolutif récurrent (RCNN)\n",
        "\n",
        "**Autres variantes de réseaux de neurones profonds**\n",
        "\n",
        "Implémentons ces modèles et comprenons leurs détails. La fonction suivante est une fonction utilitaire qui peut être utilisée pour entraîner un modèle. Il accepte le classificateur, feature_vector des données d'entraînement, les étiquettes des données d'entraînement et les vecteurs de caractéristiques des données valides comme entrées. À l'aide de ces entrées, le modèle est entraîné et le score de précision est calculé."
      ],
      "metadata": {
        "id": "2M5eoN2VAuHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "    # fit the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "\n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "\n",
        "    if is_neural_net:\n",
        "        predictions = predictions.argmax(axis=-1)\n",
        "\n",
        "    return metrics.accuracy_score(predictions, valid_y)"
      ],
      "metadata": {
        "id": "neD0HurdA_3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1 Bayes naïfs**\n",
        "\n",
        "Implémentation d'un modèle bayésien naïf à l'aide de l'implémentation sklearn avec différentes fonctionnalités\n",
        "\n",
        "Naive Bayes est une technique de classification basée sur le théorème de Bayes avec une hypothèse d'indépendance entre les prédicteurs. Un classificateur Naive Bayes suppose que la présence d'une fonctionnalité particulière dans une classe n'est pas liée à la présence d'une autre fonctionnalité"
      ],
      "metadata": {
        "id": "AFUA7si7BZ01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes on Count Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "print (\"NB, Count Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print (\"NB, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"NB, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Character Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print( \"NB, CharLevel Vectors: \", accuracy)"
      ],
      "metadata": {
        "id": "uTCqosh9Bczk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2 Classificateur linéaire**\n",
        "\n",
        "Implémentation d'un classificateur linéaire (régression logistique)\n",
        "\n",
        "La régression logistique mesure la relation entre la variable dépendante catégorielle et une ou plusieurs variables indépendantes en estimant les probabilités à l'aide d'une fonction logistique/sigmoïde. On peut en savoir plus sur la régression logistique"
      ],
      "metadata": {
        "id": "rCOIpQvIB_1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Classifier on Count Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"LR, Count Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print (\"LR, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Character Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print( \"LR, CharLevel Vectors: \", accuracy)"
      ],
      "metadata": {
        "id": "Arc1PemACnJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3 Implémentation d'un modèle SVM**\n",
        "\n",
        "Support Vector Machine (SVM) est un algorithme d'apprentissage automatique supervisé qui peut être utilisé à la fois pour des défis de classification ou de régression. Le modèle extrait le meilleur hyperplan/ligne possible qui sépare les deux classes"
      ],
      "metadata": {
        "id": "97bqTNPjDiqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print (\"SVM, N-Gram Vectors: \", accuracy)"
      ],
      "metadata": {
        "id": "gEf_E72CDw96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.4 Modèle d'ensachage = Bagging Model**\n",
        "\n",
        "Implémentation d'un modèle de forêt aléatoire\n",
        "\n",
        "Les modèles Random Forest sont un type de modèles d’ensemble, en particulier les modèles d’ensachage. Ils font partie de la famille des modèles arborescents. On peut en savoir plus sur l'ensachage et les forêts aléatoires"
      ],
      "metadata": {
        "id": "jj2MAaqyD83g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RF on Count Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
        "print (\"RF, Count Vectors: \", accuracy)\n",
        "\n",
        "# RF on Word Level TF IDF Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print (\"RF, WordLevel TF-IDF: \", accuracy)"
      ],
      "metadata": {
        "id": "rNvNTGFJEB5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.5 Modèle de renforcement = Boosting Model**\n",
        "\n",
        "Implémentation du modèle d'amplification de gradient Xtereme\n",
        "\n",
        "Les modèles de boosting sont un autre type de modèles d’ensemble faisant partie des modèles arborescents. Boosting est un méta-algorithme d'ensemble d'apprentissage automatique destiné principalement à réduire les biais, ainsi que la variance dans l'apprentissage supervisé, et une famille d'algorithmes d'apprentissage automatique qui convertissent les apprenants faibles en apprenants forts. Un apprenant faible est défini comme un classificateur qui n'est que légèrement corrélé à la véritable classification (il peut mieux étiqueter les exemples que les devinettes aléatoires). En savoir plus sur ces modèles"
      ],
      "metadata": {
        "id": "mQ_orVcaEI2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extereme Gradient Boosting on Count Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
        "print( \"Xgb, Count Vectors: \", accuracy)\n",
        "\n",
        "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
        "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
        "print (\"Xgb, CharLevel Vectors: \", accuracy)"
      ],
      "metadata": {
        "id": "4bIedeyQEcLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.6 Réseaux de neurones peu profonds**\n",
        "\n",
        "Un réseau de neurones est un modèle mathématique conçu pour se comporter de manière similaire aux neurones biologiques et au système nerveux. Ces modèles sont utilisés pour reconnaître des modèles et des relations complexes qui existent au sein de données étiquetées. Un réseau de neurones peu profond contient principalement trois types de couches : la couche d'entrée, la couche cachée et la couche de sortie. En savoir plus sur les réseaux de neurones"
      ],
      "metadata": {
        "id": "PQ2KlbBxFIIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_architecture(input_size):\n",
        "    # create input layer\n",
        "    input_layer = layers.Input((input_size, ), sparse=True)\n",
        "\n",
        "    # create hidden layer\n",
        "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
        "\n",
        "    # create output layer\n",
        "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    return classifier\n",
        "\n",
        "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
        "accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
        "print (\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
      ],
      "metadata": {
        "id": "iBXXGPI9FqmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.7 Réseaux de neurones profonds**\n",
        "\n",
        "Les réseaux de neurones profonds sont des réseaux de neurones plus complexes dans lesquels les couches cachées effectuent des opérations beaucoup plus complexes que de simples activations sigmoïdes ou relu. Différents types de modèles d’apprentissage profond peuvent être appliqués aux problèmes de classification de textes.\n",
        "\n"
      ],
      "metadata": {
        "id": "hhUKCOlTGJfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*3.7.1 Réseau neuronal convolutif*\n",
        "\n",
        "Dans les réseaux de neurones convolutifs, les convolutions sur la couche d'entrée sont utilisées pour calculer la sortie. Il en résulte des connexions locales, où chaque région de l'entrée est connectée à un neurone de la sortie. Chaque couche applique différents filtres et combine leurs résultats."
      ],
      "metadata": {
        "id": "5_5BWSeEHIyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "\n",
        "    return model\n",
        "\n",
        "classifier = create_cnn()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "print (\"CNN, Word Embeddings\",  accuracy)"
      ],
      "metadata": {
        "id": "CYG8GpkIGMDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*3.7.2 Réseau neuronal récurrent – ​​LSTM*\n",
        "\n",
        "Contrairement aux réseaux de neurones Feed-forward dans lesquels les sorties d'activation se propagent dans une seule direction, les sorties d'activation des neurones se propagent dans les deux sens (des entrées aux sorties et des sorties aux entrées) dans les réseaux de neurones récurrents. Cela crée des boucles dans l'architecture du réseau neuronal qui agit comme un « état mémoire » des neurones. Cet état permet aux neurones de se souvenir de ce qui a été appris jusqu'à présent.\n",
        "\n",
        "L'état de la mémoire dans les RNN donne un avantage par rapport aux réseaux de neurones traditionnels, mais un problème appelé Vanishing Gradient leur est associé. Dans ce problème, lors de l’apprentissage avec un grand nombre de couches, il devient très difficile pour le réseau d’apprendre et d’ajuster les paramètres des couches précédentes. Pour résoudre ce problème, un nouveau type de RNN appelés modèles LSTM (Long Short Term Memory) a été développé."
      ],
      "metadata": {
        "id": "WMKJSzvRMjx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rnn_lstm():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "\n",
        "    return model\n",
        "\n",
        "classifier = create_rnn_lstm()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "print(\"RNN-LSTM, Word Embeddings\",  accuracy)"
      ],
      "metadata": {
        "id": "tXbVka2XMkVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*3.7.3 Réseau neuronal récurrent – ​​GRU*\n",
        "\n",
        "Les unités récurrentes fermées sont une autre forme de réseaux neuronaux récurrents. Ajoutons une couche de GRU au lieu de LSTM dans notre réseau."
      ],
      "metadata": {
        "id": "RkAK3Dy6MlMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rnn_gru():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the GRU Layer\n",
        "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "\n",
        "    return model\n",
        "\n",
        "classifier = create_rnn_gru()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "print(\"RNN-GRU, Word Embeddings\",  accuracy)"
      ],
      "metadata": {
        "id": "gmBDoaRaMlxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*3.7.4 RNN bidirectionnel*\n",
        "\n",
        "Les couches RNN peuvent également être enveloppées dans des couches bidirectionnelles. Enveloppons notre couche GRU dans une couche bidirectionnelle."
      ],
      "metadata": {
        "id": "P-NK4auDMsn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bidirectional_rnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "\n",
        "    return model\n",
        "\n",
        "classifier = create_bidirectional_rnn()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "print(\"RNN-Bidirectional, Word Embeddings\",  accuracy)"
      ],
      "metadata": {
        "id": "MISKb3BqMtJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*3.7.5 Réseau neuronal convolutif récurrent*\n",
        "\n",
        "Une fois les architectures essentielles essayées, on peut essayer différentes variantes de ces couches comme le réseau neuronal convolutif récurrent. D'autres variantes peuvent être :\n",
        "\n",
        "Réseaux d'attention hiérarchique\n",
        "\n",
        "Modèles séquence à séquence avec attention\n",
        "\n",
        "Réseaux de neurones convolutifs récurrents bidirectionnels\n",
        "\n",
        "CNN et RNN avec plus de couches"
      ],
      "metadata": {
        "id": "ma4f5MRxM5DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rcnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the recurrent layer\n",
        "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
        "\n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "\n",
        "    return model\n",
        "\n",
        "classifier = create_rcnn()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "print(\"CNN, Word Embeddings\",  accuracy)"
      ],
      "metadata": {
        "id": "cWrjWM2SM5in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Améliorer les modèles de classification de texte\n",
        "\n",
        "Bien que le cadre ci-dessus puisse être appliqué à un certain nombre de problèmes de classification de texte, pour obtenir une bonne précision, certaines améliorations peuvent être apportées au cadre global. Par exemple, voici quelques conseils pour améliorer les performances des modèles de classification de texte et de ce cadre.\n",
        "\n",
        "1. Nettoyage de texte :  le nettoyage de texte peut aider à réduire le bruit présent dans les données textuelles sous forme de mots vides, de signes de ponctuation, de variations de suffixes, etc. Cet  article  peut aider à comprendre comment implémenter la classification de texte en détail.\n",
        "\n",
        "2. Fonctionnalités Hstacking Text / NLP avec des vecteurs de fonctionnalités de texte :  dans la section d'ingénierie des fonctionnalités, nous avons généré un certain nombre de vecteurs de fonctionnalités différents, leur combinaison peut aider à améliorer la précision du classificateur.\n",
        "\n",
        "3. Réglage des hyperparamètres dans la modélisation :  Le réglage des paramètres est une étape importante, un certain nombre de paramètres tels que la longueur de l'arbre, les feuilles, les paramètres de réseau, etc. peuvent être ajustés pour obtenir le modèle le mieux adapté.\n",
        "\n",
        "4. Modèles d'ensemble :  empiler différents modèles et mélanger leurs sorties peut aider à améliorer encore les résultats."
      ],
      "metadata": {
        "id": "9xEOWsKbOkQg"
      }
    }
  ]
}